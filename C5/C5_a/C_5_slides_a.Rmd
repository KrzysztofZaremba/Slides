---
title: 'Class 5a: Multiple Linear Regression'
author: "Business Forecasting"
output:
  xaringan::moon_reader:
    self_contained: true
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: true
      
---   
<style type="text/css">
.remark-slide-content {
    font-size: 20px;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dpi=300)
library(shiny)
library(ggplot2)
library(forecast)
library(plotly)
library(dplyr)
library(igraph)
library(reshape)
library(spData)
library(leaflet)
library(readr)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(hrbrthemes)
library(gridExtra)
library(cowplot)
library(viridis)
library(gapminder)
library(knitr)
library(car)
library(kableExtra)
library(DT)

```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(base_color = "#43418A", 
colors = c(
  red = "#f34213",
  purple = "#3e2f5b",
  orange = "#ff8811",
  green = "#136f63",
  blue = "#1E90FF",
  white = "#FFFFFF"
))
```


```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(dplyr)
library(MASS) # for truehist function
load("URG_Sample.Rda")
Sample_urg=Sample_urg[Sample_urg$SEXO!="NO ESPECIFICADO",]
```
---
## Roadmap


### This set of classes
- What is a multiple linear regression


---
### Motivation

- Suppose that you are administering a hospital
- You need to know how many doctors, nurses and beds you need
- So you want to predict how long a patient will stay at the urgent care

--
- You collect the data on 
    - The Duration of the visit
    - The type of patient
    - How many other people there are currently at urgent care
    - What kind of problem they came with
    - What type of bed they got

--
- If we know these factors, can we predict how long patient will stay?

---
### Data


```{r, echo=FALSE}
# Load the DT package
library(DT)


# Display the data table
datatable(Sample_urg,
          fillContainer = FALSE,
          options = list(
            pageLength = 10,
            searching = FALSE,
            initComplete = JS(
              "function(settings, json) {",
              "$(this.api().table().container()).css({'font-size': '12px'});",
              "}"
            )
          ),
          rownames = FALSE
)
```


---

### Multiple linear regression

Suppose that the outcome $y_i$ (duration) is a linear function of $x_1$ (occupancy) and $x_2$ (age)

$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+u_i$$
- $\beta_0$ represents the value of $y_i$ when $x_1$ and $x_2$ are 0. 
- $\beta_1$ represents the change in $y_i$ while changing $x_1$ by one unit and keeping $x_2$ constant
- $\beta_2$ represents the change in $y_i$ while changing $x_2$ by one unit and keeping $x_1$ constant

---
### Multiple linear regression

100 observations simulated from an a regression line:
$$y_i=5+2x_{i1}+1x_{i2}+u_i$$
```{r, warning=FALSE,message=FALSE,fig.height=3, out.width='80%'}

library(plotly)
library(reshape2)

# Simulate your data
set.seed(123)
n <- 100
x1 <- rnorm(n)
x2 <- rnorm(n)
u <- rnorm(n)
y <- 5 + 2 * x1 + x2 + u
my_df <- data.frame(x1 = x1, x2 = x2, y = y)

# Fit a multiple linear regression model
# Load the necessary packages for 3D plotting
lm_model <- lm(y ~ x1 + x2, data = my_df)

# Create a grid of values for x1 and x2
x1_grid <- seq(min(my_df$x1), max(my_df$x1), length.out = 50)
x2_grid <- seq(min(my_df$x2), max(my_df$x2), length.out = 50)
grid_data <- expand.grid(x1 = x1_grid, x2 = x2_grid)

# Predict y values using the regression model
grid_data$y_pred <- predict(lm_model, newdata = grid_data)

# Convert the grid data to a matrix for persp plot
z_matrix <- matrix(grid_data$y_pred, nrow = length(x1_grid), ncol = length(x2_grid), byrow = TRUE)

# Create the base 3D scatter plot (points)
plot_ly(my_df, x = ~x1, y = ~x2, z = ~y, text = ~paste("x1:", round(x1,2), "<br>x2:", round(x2,2), "<br>y:", round(y,2)),
        hoverinfo = 'text',
        type = "scatter3d", mode = "markers", height = 400) %>%
  layout(scene = list(aspectmode = "cube")) %>%

# Add the surface using persp
  add_surface(z = ~z_matrix, x = x1_grid, y = x2_grid, 
        hoverinfo = 'text',
        type = "surface", opacity = 0.6, text = ~paste("x1:", x1_grid, "<br>x2:", x2_grid, "<br>y_pred:", z_matrix))
```


---
### Multiple linear regression

100 observations simulated from an a regression line: $$y_i=5+2x_{i}-1x_i^2+u_i$$
```{r, warning=FALSE,message=FALSE,fig.height=4, out.width='100%'}
set.seed(123)

# Number of data points
n <- 100

# Generate random values for x1, x, and u
x1 <- rnorm(n)
x <- rnorm(n, mean=1, s=3)
u <- rnorm(n, s=10)

# Generate y using the given equation
y <- 5 + 2 * x1 - x^2 + u

# Create a data frame
data <- data.frame(y, x1, x)

# Perform linear regression
lm_model <- lm(y ~ x1 + I(x^2), data=data)

# Print the summary of the regression

# Load the ggplot2 library
library(ggplot2)

# Create a scatter plot with regression line
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), se = FALSE, color = "blue") +
  theme_xaringan()

```


---
### Multiple linear regression

Suppose that: 
$$x_1 = \begin{cases}
    1 & \text{if female} \\
    0 & \text{if male}
\end{cases}$$

100 observations simulated from an a regression line: $$y_i=5+2x_{i1}-1x_{i2}+u_i$$
```{r, warning=FALSE,message=FALSE,fig.height=3, out.width='100%'}
set.seed(123)

# Number of data points
n <- 100

# Generate random values for x1, x2, and u
x1 <- sample(c(0, 1), n, replace = TRUE)  # Binary variable (0 or 1)
x2 <- rnorm(n)
u <- rnorm(n)

# Generate y using the given equation
y <- 5 + 2 * x1 - x2 + u

# Create a data frame
data <- data.frame(y, x1, x2)

# Load the ggplot2 library
library(ggplot2)

# Separate data for males and females
data_male <- subset(data, x1 == 0)
data_female <- subset(data, x1 == 1)

# Create a scatter plot with parallel regression lines for males and females
plot <- ggplot(data, aes(x = x2, y = y, color = factor(x1))) +
  geom_point() +
  labs( x = "x2",
       y = "y",
       color = "x1") +
  scale_color_manual(values = c("0" = "blue", "1" = "red")) +
  
  # Add parallel regression lines for males and females
  geom_line(data = data, aes(x = x2, y = 5 + 2 * 0 - x2), color = "blue") +
  geom_line(data = data, aes(x = x2, y = 5 + 2 * 1 - x2), color = "red")+
  theme_xaringan()

  
  plot
```



---
### Multiple linear regression


Now imagine a regression with k variables:

$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+...+\beta_kx_{ik}+u_i$$
- Maybe you are trying to predict customer spending based on what they looked at and $x_{ij}$ represent how long customer $i$ looked at item $j$

--
- Maybe you are trying to predict sales in a store $i$, and $x_{ij}$ represent prices of the products, their competitors' products, how many people live around and how rich are they etc...

--
- We can no longer visualize it (because we can't visualize more than 3 dimensions) 

---
### Multiple linear regression

We can also write it in the vector form:

$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+...+\beta_k,x_{ik}+u_i$$
In vector form is: 



$$\mathbf{y}=\mathbf{X\beta}+\mathbf{u}$$

<div class="math">
\[
\underbrace{\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n \\ \end{bmatrix}}_{\substack{\mathbf{y} \\ n \times 1}}
=
\underbrace{\begin{bmatrix}
1 & x_{11} & x_{12} & ... & x_{1k} \\
1 & x_{21} & x_{22} & ... & x_{2k}  \\
\vdots & \vdots & \vdots & ....& \vdots \\
1 & x_{n1} & x_{n2} & ... & x_{nk}  &  \end{bmatrix}}_{\substack{\mathbf{X} \\ n \times (k+1)}}

\underbrace{\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_k  \\ \end{bmatrix}}_{\substack{\mathbf{\beta} \\ (k+1) \times 1}}
+
\underbrace{\begin{bmatrix}
u_1 \\
u_2 \\
\vdots \\
u_n  \\ \end{bmatrix}}_{\substack{\mathbf{u} \\ n \times 1}}
\]
</div>

---
### Full Rank

Important Assumption: **X is full rank** 
- Has same rank as the number of parameters: $p=k+1$
- Also known as: no perfect multicolinearity

--
- .blue[Technically]: columns of X should be linearly independent

--
- .blue[Intuitively]: none of the variables are perfectly correlated. If they are perfectly correlated, then we don't need one of the columns because we can perfectly predict one column with information from another column.

- Suppose that one column is income in USD, and the second one is income measured in Pesos. They are perfectly correlated. Once we know income in USD, income in Pesos does not bring any additional information. We would not be able to estimate the effect of both income in USD and income in Pesos at the same time. 

<div class="math">
\[
\begin{array}{cc}
\text{Full Rank Matrix:} & \text{Matrix Not of Full Rank:} \\
\left[\begin{array}{ccc}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{array}\right]
&
\left[\begin{array}{ccc}
1 & 2 & 4 \\
4 & 5 & 10 \\
7 & 8 & 16
\end{array}\right]
\end{array}
\]
</div>



---
### Multiple linear regression

**Goal:**
- Estimate the vector of parameters $\mathbf{\beta}$




**Procedure**
- Find 
<div class="math">
\[
\mathbf{b}=\begin{bmatrix}
b_0 \\
b_1 \\
\vdots \\
b_k  \\ \end{bmatrix}
\]
</div>

- Which minimizes the squared errors in the problem: 

$$y_i=b_0+b_1x_{i1}+b_2x_{i2}+...+b_kx_{ik}+e_i$$
- That is minimize 

$$SSE=\sum_ie_i^2=\sum_i(y_i-\hat{y}_i)^2=\mathbf{e}'\mathbf{e}=(\mathbf{y-\hat{y}})'(\mathbf{y-\hat{y}})=(\mathbf{y-Xb})'(\mathbf{y-Xb})$$
---
### Multiple linear regression

- We can do it with scalars

$$\begin{align*}
\frac{\partial SSE}{\partial \hat{\beta}_0} & = -2\sum_{i=1}^{n} \left( y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_{i1}+...+\hat{\beta}_k x_{ik})\right)=0 \\
\frac{\partial SSE}{\partial \hat{\beta}_1} & = -2\sum_{i=1}^{n} x_{i1} \left( y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_{i1}+...+\hat{\beta}_k x_{ik})\right)=0 \\
\vdots \\
\frac{\partial SSE}{\partial \hat{\beta}_k} & = -2\sum_{i=1}^{n} x_{ik} \left( y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_{i1}+...+\hat{\beta}_k x_{ik})\right)=0 \\
\end{align*}$$

--
- We have $k+1$ equations with $k+1$ unknowns. 

---
### Multiple linear regression

- Or we can do it with vectors

--
- First rewrite the sum of squares:
$$SSE(b)=(\mathbf{y-Xb})'(\mathbf{y-Xb})=\mathbf{y'}\mathbf{y-2b'X'y}+\mathbf{b'X'Xb}$$

--
- Then minimize it with respect to $\mathbf{b}$

$$\frac{\partial}{\partial \mathbf{b}}(\mathbf{y'}\mathbf{y-2b'X'y}+\mathbf{b'X'Xb})=\mathbf{-2X'y}+\mathbf{2X'Xb}$$

--
- $\hat{\beta}$ is the solution of such minimization (our OLS estimator)

$$\begin{align*} 
\mathbf{-2X'y}+\mathbf{2X'X\hat{\beta}}&=0 \\
\mathbf{X'X\hat{\beta}} & =\mathbf{X'y} \\
\mathbf{\hat{\beta}} & =\mathbf{(X'X)^{-1}X'y}
\end{align*}$$


---
### Multiple linear regression

Looking more closely at the **first order condition**:

<div class="math">
\[
\underbrace{\begin{bmatrix}
n & \sum_{i=1}^{n}x_{i1} & \ldots & \sum_{i=1}^{n} x_{ik} \\
\sum_{i=1}^{n} x_{i1} & \sum_{i=1}^{n} x_{i1}^2 & \ldots & \sum_{i=1}^{n} x_{i1}x_{ik} \\
\vdots & \vdots & \ddots & \vdots \\
\sum_{i=1}^{n} x_{ik}& \sum_{i=1}^{n} x_{ik}x_{i1} & \ldots & \sum_{i=1}^{n} x_{ik}^2\end{bmatrix}}_{\mathbf{X'X}}
\underbrace{\begin{bmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1 \\
\vdots \\
\hat{\beta}_k\end{bmatrix}}_{\hat{\beta}}
=
\underbrace{\begin{bmatrix}
\sum_{i=1}^{n}y_i \\
\sum_{i=1}^{n}x_{i1}y_i \\
\vdots \\
\sum_{i=1}^{n}x_{ik}y_i\end{bmatrix}}_{\mathbf{X'y}}
\]
</div>

Looking more closely and it's **solution**:


<div class="math">
\[\underbrace{\begin{bmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1 \\
\vdots \\
\hat{\beta}_k\end{bmatrix}}_{\hat{\beta}}
=
\underbrace{\begin{bmatrix}
n & \sum_{i=1}^{n}x_{i1} & \ldots & \sum_{i=1}^{n} x_{ik} \\
\sum_{i=1}^{n} x_{i1} & \sum_{i=1}^{n} x_{i1}^2 & \ldots & \sum_{i=1}^{n} x_{i1}x_{ik} \\
\vdots & \vdots & \ddots & \vdots \\
\sum_{i=1}^{n} x_{ik}& \sum_{i=1}^{n} x_{ik}x_{i1} & \ldots & \sum_{i=1}^{n} x_{ik}^2\end{bmatrix}^{-1}}_{\mathbf{(X'X)}^{-1}}

\underbrace{\begin{bmatrix}
\sum_{i=1}^{n}y_i \\
\sum_{i=1}^{n}x_{i1}y_i \\
\vdots \\
\sum_{i=1}^{n}x_{ik}y_i\end{bmatrix}}_{\mathbf{X'y}}
\]
</div>

---
### Practice theory

Lista 5.1 Q9

---

### Special Case: k=1

What if we have just one $x$?

<div class="math">
\[\underbrace{\begin{bmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1 \end{bmatrix}}_{\hat{\beta}}
=
\underbrace{\begin{bmatrix}
n & \sum_{i=1}^{n}x_{i1} \\
\sum_{i=1}^{n} x_{i1} & \sum_{i=1}^{n} x_{i1}^2\end{bmatrix}^{-1}}_{\mathbf{(X'X)}^{-1}}


\underbrace{\begin{bmatrix}
\sum_{i=1}^{n}y_i \\
\sum_{i=1}^{n}x_{i1}y_i \end{bmatrix}}_{\mathbf{X'y}}
\]
</div>

--
<div class="math">
\[
\begin{bmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1\end{bmatrix}
=
\begin{bmatrix}
\frac{\sum^n_{i=1}x_{i1}^2}{n\sum_{i=1}^{n} x_{i1}^2 - (\sum_{i=1}^{n} x_{i1})^2} & \frac{-\sum_{i=1}^{n} x_{i1}}{n\sum_{i=1}^{n} x_{i1}^2 - (\sum_{i=1}^{n} x_{i1})^2} \\
\frac{-\sum_{i=1}^{n} x_{i1}}{n\sum_{i=1}^{n} x_{i1}^2 - (\sum_{i=1}^{n} x_{i1})^2} & \frac{n}{n\sum_{i=1}^{n} x_{i1}^2 - (\sum_{i=1}^{n} x_{i1})^2}\end{bmatrix}
\begin{bmatrix}
\sum_{i=1}^{n}y_i \\
\sum_{i=1}^{n}x_{i1}y_i \end{bmatrix}
\]
</div>

--
which gives:

<div class="math">
\[
\begin{bmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1\end{bmatrix}
=
\begin{bmatrix} \bar{y}-\bar{x}_1\frac{\sum(x_{1i}y_i-n\bar{y}\bar{x}_1)}{\sum_{i=1}^{n} x_{i1}^2 - n\bar{x}_{1}^2} \\
\frac{\sum_ix_{1i}y_i-n\bar{x}_{1}\bar{y}}{\sum_{i=1}^{n} x_{i1}^2 - n\bar{x}_{1}^2}\end{bmatrix}
\]
</div>

---
### Predictions
 
To make predictions based on the estimated regressors we use:

$$\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_{i1}+\hat{\beta}_2x_{i2}+...+\hat{\beta}_kx_{ik}$$ 
Or in the vector form:


$$\mathbf{\hat{y}}=\mathbf{X\hat{\beta}}=\mathbf{X\mathbf{(X'X)}^{-1}\mathbf{X'y}}=\mathbf{Hy}$$

Where $\mathbf{H}=\mathbf{X(X'X)}^{-1}\mathbf{X}$ is called a hat matrix. 

---
### Residuals

To get residuals, we calculate: 

$$e_i=y_i-\hat{y}_i=y_i-\hat{\beta}_0+\hat{\beta}_1x_{i1}+\hat{\beta}_2x_{i2}+...+\hat{\beta}_kx_{ik}$$ 

Or in the vector form:
$$\mathbf{e}=\mathbf{y-\hat{y}}=y-\mathbf{X\hat{\beta}}=\mathbf{y}-\mathbf{X\mathbf{(X'X)}^{-1}\mathbf{X'y}}=\mathbf{(I-H)y}$$
---

### Practice Theory
Lista 5.1 Q7

---

#### Example with numbers
Similar to Lista 5.1 Q8

$$\small \begin{align*}
\text{Dataset:} \\
&\begin{array}{|c|c|c|c|}
\hline
\text{Student} & \text{Hours Studied (}x_1\text{)} & \text{Hours Slept (}x_2\text{)} & \text{Exam Score (}y\text{)} \\
\hline
1 & 3 & 8 & 80 \\
2 & 4 & 7 & 85 \\
3 & 6 & 6 & 92 \\
4 & 5 & 7 & 88 \\
\hline
\end{array}
\\
\text{X matrix}: \\
& X = \begin{bmatrix}
1 & 3 & 8 \\
1 & 4 & 7 \\
1 & 6 & 6 \\
1 & 5 & 7 \\
\end{bmatrix}
\\
\text{Response Vector } (y): \\
& y = \begin{bmatrix}
80 \\
85 \\
92 \\
88 \\
\end{bmatrix}
\end{align*}$$

We are trying to find:

$$\mathbf{\hat{\beta}} =\mathbf{(X'X)^{-1}X'y}$$

---

#### Example with numbers

Multiply $X'$ by $X$:

$$X'X = 
\begin{bmatrix}
1 & 1 & 1 & 1 \\
3 & 4 & 6 & 5 \\
8 & 7 & 6 & 7 \\
\end{bmatrix}
\begin{bmatrix}
1 & 3 & 8 \\
1 & 4 & 7 \\
1 & 6 & 6 \\
1 & 5 & 7 \\
\end{bmatrix}=
\begin{bmatrix}
4 & 18 & 28 \\
18 & 86 & 123 \\
28 & 123 & 198 \\
\end{bmatrix}$$

Find the inverse $(X'X)^{-1}$

$$(X'X)^{-1} = 
\begin{bmatrix}
474.75 & -30 & -48.5 \\
-30 & 2 & 3 \\
-48.5 & 3 & 5 \\
\end{bmatrix}$$


---
#### Example with numbers

Next let's find $X'y$

$$X'y=
\begin{bmatrix}
1 & 1 & 1 & 1 \\
3 & 4 & 6 & 5 \\
8 & 7 & 6 & 7 \\
\end{bmatrix}
\begin{bmatrix}
80 \\
85 \\
92 \\
88 \\
\end{bmatrix}=
\begin{bmatrix}
345 \\
1572 \\
2403 \\
\end{bmatrix}$$
So, our coefficients are:


$$\beta=\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\end{bmatrix} =
 \underbrace{\begin{bmatrix}
474.75 & -30 & -48.5 \\
-30 & 2 & 3 \\
-48.5 & 3 & 5 \\
\end{bmatrix}}_{(X'X)^{-1}} \underbrace{\begin{bmatrix}
345 \\
1572 \\
2403 \\
\end{bmatrix}}_{X'y}=
\begin{bmatrix}
83.25 \\
3 \\
-1.5 \\
\end{bmatrix}$$


--

**Interpretation**
- Score with 0 hours of sleep and 0 of studying is 83.25
- 1 more hour of studying (without changing sleep hours) increases score by 3 
- 1 more hour of sleep (without changing study hours) decreases score by 1.5 


---
#### Example with numbers

We can find predicted values:
$$\hat{y}=X\hat{\beta}=
\begin{bmatrix}
1 & 3 & 8 \\
1 & 4 & 7 \\
1 & 6 & 6 \\
1 & 5 & 7 \\
\end{bmatrix}\begin{bmatrix}
83.25 \\
3 \\
-1.5 \\
\end{bmatrix}=
\begin{bmatrix}
80.25 \\
84.75 \\
92.25 \\
87.75
\\
\end{bmatrix}$$
And the residuals:
$$e=y-\hat{y}=y-X\hat{\beta}=
\begin{bmatrix}
80 \\
85 \\
92 \\
88 \\
\end{bmatrix}-
\begin{bmatrix}
80.25 \\
84.75 \\
92.25 \\
87.75
\end{bmatrix}=
\begin{bmatrix}
-0.25 \\
0.25 \\
-0.25 \\
0.25
\end{bmatrix}$$



---
#### Example from data: 
```{r, echo=TRUE}
# Fit a linear regression model
lm_model <- lm(Duration ~ Occupancy+EDAD, data = Sample_urg)
# Display the summary of the linear regression model
summary(lm_model)

```

---

### Practice

Predict how long a patient will stay if there 10 other patients in the hospital and the patient is 50 years old.

---

### Correlations vs Coefficients

Note, that $x_1$ and $x_2$ can both have positive correlation with $y_i$, but different coefficients!

- Suppose $x_1$ is study hours, $x_2$ is coffee cups drunk by a student, and $y$ is student's score on the exam. 


```{r, warning=FALSE,message=FALSE,fig.height=3, out.width='1000%'}
library(GGally)

# Create a dataset
set.seed(123)
n <- 100
x1 <- runif(n, 0, 10)  # hours spent studying
x2 <- x1 + rnorm(n, 0, 2)  # cups of coffee, correlated with study hours
y <- 3 + 2*x1 + rnorm(n, 0, 2)  # exam score

# Create a data frame
data <- data.frame(x1, x2, y)

# Create a matrix of scatterplots
ggpairs(data)+theme_xaringan()

```

---

### Correlations vs Coefficients

```{r, warning=FALSE,message=FALSE,fig.height=3, out.width='80%'}
# Fit a linear regression model
fit <- lm(y ~ x1 + x2, data)
# Display the summary of the linear regression model
summary(fit)

```

--
- Why coffee has 0 impact?

--
- Because it only helps to study longer, but comparing students who study the same amount, drinking more coffee is not better. 
---
### OLS Properties

- As usual, we asked whether it's unbiased and what is its variance.

--

- **Unbiased**:

<div class="math">
\[\small
\begin{align*} 
E(\hat{\beta}) & =E(\mathbf{(X'X)^{-1}X'y})=E(\mathbf{(X'X)^{-1}X'(X\beta+u)})) \\
& = E(\mathbf{(X'X)^{-1}X'(X\beta+u)}))=E(\mathbf{(X'X)^{-1}X'X\beta})+E(\mathbf{(X'X)^{-1}X'u}) \\
& = \beta+0=\beta
\end{align*}
\]
</div>

Where $\small E(\mathbf{(X'X)^{-1}X'u})$ if $\small E(u|X)=0$ (our usual assumption).

--

- **Variance**

$$\small Var(\hat{\beta})=Cov(\hat{\beta})=\underbrace{\begin{bmatrix}
var(\hat{\beta_0}) & cov(\hat{\beta_0}, \hat{\beta_1}) & ... & cov(\hat{\beta_0}, \hat{\beta_k}) \\
cov(\hat{\beta_1}, \hat{\beta_0}) & var(\hat{\beta_1}) & ... & cov(\hat{\beta_1}, \hat{\beta_k})  \\
\vdots & \vdots & \vdots &  \vdots \\
cov(\hat{\beta_k}, \hat{\beta_0}) & cov(\hat{\beta_k}, \hat{\beta_1}) & ... & var(\hat{\beta_k}) &  \end{bmatrix}}_{\substack{ \\ (k+1) \times (k+1)}}$$
- So it's a matrix with variance of single parameters on the diagonal and covariances off the diagonal.

---
### Variance

First, note that: 

$$\hat{\beta} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X}\beta + \mathbf{u} = \beta + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{u}$$
Let's use this

<div class="math">
\[\small
\begin{align*} 
var(\hat{\beta}) &  = \mathbb{E}[(\hat{\beta} - \mathbb{E}[\hat{\beta}]) (\hat{\beta} - \mathbb{E}[\hat{\beta}])']
 \\
& =  \mathbb{E}[(X'X)^{-1}X'\mathbf{u} ((X'X)^{-1}X'\mathbf{u})']= (X'X)^{-1}X'\mathbb{E}[\mathbf{u}\mathbf{u}']X(X'X)^{-1}
 \\
& = (X'X)^{-1}X'(I\sigma^2)X(X'X)^{-1} =\sigma^2(X'X)^{-1}
\end{align*}
\]
</div>

--

So $$var(\hat{\beta}_k)=\sigma^2(X'X)^{-1}_{k+1,k+1}$$ where $(X'X)^{-1}_{k+1,k+1}$ is element in $k$ row and $k$ column of $(X'X)^{-1}$ matrix.

--
- Because first coefficient is $\beta_0$

--

- And standard deviation is just square root of this!


---
### Variance

- Where the hell do we get the $\sigma^2$ from?!


--
- Same as before: 

$$\hat{\sigma}^2=\frac{\sum_i e_i^2}{n-p}$$
- Where $e_i$ is fitted residual and $p$ is number of parameters $p=k+1$
- This is called mean squared error as well

--

The easiest way to compute this sum is:

$$\sum_i e_i^2=\mathbf{e'}\mathbf{e}=(\mathbf{y-X\hat{\beta}})'(\mathbf{y-X\hat{\beta}})=\mathbf{y'y-\hat{\beta}'X'y}$$

---

#### Gauss Markov Theorem (Again)

Assumptions

- $E(u_i)=0$
- $var(u_i)=\sigma^2$
- $cov(u_i,u_j)=0$
- $X$ is full rank

NO NEED FOR NORMALITY



**Theorem:** OLS is .blue[BLUE:] Best, Linear, Unbiased Estimator

- It has the lowest variance among linear and unbiased estimators

--

- What's a linear estimator?
  - It's an estimator where $\beta$ coefficients are linear functions of outcomes
  - Anything of the form $b=Cy$ where C is p x n matrix.
  - So $b_1=c_{11}y_1+c_{12}y_2+...+c_{13}y_3$
  - Example $b_1=\frac{1}{n}y_1+...+\frac{1}{n}y_n$

--

- How is OLS linear?
  $\hat{\beta}=Cy=\underbrace{(X'X)^{-1}X'}_{C}y$



---

### Categorical Variables in a Regression

- Suppose we want to learn whether mode of work affects workers productivity.
- Each worker can be in one of these 3 categories:
  - Fully at the office
  - Fully remote
  - Hybrid

```{r, echo=FALSE}
# Load the DT package
library(DT)

workers <- 100  # Number of workers
WorkMode <- factor(sample(c("Fully at the office", "Fully remote", "Hybrid"), 
                         workers, replace = TRUE),
                   levels = c("Fully at the office", "Fully remote", "Hybrid"))
Productivity <- round(runif(workers, 70, 130))  # Random productivity scores between 70 and 130
WorkerID=1:100

d=data.frame(WorkerID, Productivity, WorkMode)



# Display the data table
datatable(d,
          fillContainer = FALSE,
          options = list(
            pageLength = 8,
            searching = FALSE,
            initComplete = JS(
              "function(settings, json) {",
              "$(this.api().table().container()).css({'font-size': '12px'});",
              "}"
            )
          ),
          rownames = FALSE
)
```

---

- How do we estimate the impact of categorical variable?
- We turn it into a series of binary variables (or indicator variables)!

$$D_{i, Remote}=\begin{cases}
1 & WorkMode_i=Fully Remote \\
0 & otherwise
\end{cases}$$

$$D_{i,Hybrid}=\begin{cases}
1 & WorkMode_i=Hybrid\\
0 & otherwise
\end{cases}$$

```{r, echo=FALSE}
# Load the DT package
# Your existing code
library(DT)

workers <- 100  # Number of workers
WorkMode <- factor(sample(c("Fully at the office", "Fully remote", "Hybrid"), 
                         workers, replace = TRUE),
                   levels = c("Fully at the office", "Fully remote", "Hybrid"))
Productivity <- round(runif(workers, 70, 130))  # Random productivity scores between 70 and 130
WorkerID = 1:100

d = data.frame(WorkerID, Productivity, WorkMode)

# Create dummy variables for WorkMode
dummy_vars <- model.matrix(~ WorkMode - 1, data = d)

# Convert the matrix of dummy variables to a data frame
dummy_df <- data.frame(dummy_vars)

# Combine the dummy variables with your original data
d <- cbind(d, dummy_df)

# Display the updated data table with the dummy variables
datatable(d,
          fillContainer = FALSE,
          options = list(
            pageLength = 6,
            searching = FALSE,
            initComplete = JS(
              "function(settings, json) {",
              "$(this.api().table().container()).css({'font-size': '12px'});",
              "}"
            )
          ),
          rownames = FALSE
)
```

--
- For each person, only one of these dummies is equal to 1!

---
- We will add these dummies into a regression, but not all of them!

- If we have m categories, we will add m-1 dummies. Why?

$$y_i=\beta_0+\beta_1D_{i1}+\beta_2D_{i2}+...+\beta_{m-1}D_{im-1}+u_i$$

- In our Example:

$$y_i=\beta_0+\beta_1D_{i,Hybrid}+\beta_2D_{i,Remote}+u_i$$
--

- Because otherwise X would not be full rank!

<div class="math">
\[
\begin{array}{cc}
\text{Full Rank Matrix:} & \text{Matrix Not of Full Rank:} \\
\left[\begin{array}{ccc}
1 & 1 & 0 \\
1 & 0 & 0 \\
1 & 0 & 1
\end{array}\right]
&
\left[\begin{array}{cccc}
1 & 1 & 0 & 0\\
1 & 0 & 0 & 1\\
1 & 0 & 1 & 0
\end{array}\right]
\end{array}
\]
</div>

--

- Intuitively, if I know that the values of $D_{i,Hybrid}$ and $D_{i,Remote}$, I know the value of $D_{i,Office}$
- Ex: if they don't work hybrid and don't work remote, I know they work at the office
- So including it does not bring any new information

---


- R automatically transform categorical variable to dummies and excludes one of them
 

```{r, echo=TRUE}
# Fit a linear regression model
lm_model <- lm(Productivity ~ WorkMode, data = d)
# Display the summary of the linear regression model
summary(lm_model)

```


---

### Interpretation of Coefficients



- Coefficient on a dummy $D_1$ tells us by how much $y$ changes when we change category from the excluded one to the category 1.

- In our example
  - Excluded category is: work fully at the office - this is our comparision group
  - $\beta_{hybrid}=6.184$: employees working in hybrid mode have on average 6.184 higher productivity score compared to the ones working at the office
  - $\beta_{remote}=-7.256$: employees working in fully remotely have on average 7.256 lower productivity score compared to the ones working at the office
  - The t-test on these coefficients tells us whether these differences in means across categories are significant!


- Bottom line: the coefficients on the dummies show the average difference between $y$ in that category compared to the excluded category (holding everything else unchanged)

---

### Example

Suppose we have a categorical variable representing education level. We run a regression of income on the education level. Interpret the coefficients. 

```{r, echo=FALSE}
# Load the DT package
# Your existing code
library(DT)

workers <- 100  # Number of workers
Education <- factor(sample(c("High School or less", "PhD", "Master", "Bachelor"), 
                         workers, replace = TRUE),
                   levels = c("High School or less", "PhD", "Master", "Bachelor"))
Income <- round(runif(workers, 50000, 100000))  # Random productivity scores between 70 and 130
Income[Education!="High School or less"]=Income[Education!="High School or less"]+10000
Income[Education=="Master"]=Income[Education=="Master"]+10000


WorkerID = 1:100

d = data.frame(WorkerID, Income, Education)


# Display the updated data table with the dummy variables
datatable(d,
          fillContainer = FALSE,
          options = list(
            pageLength = 6,
            searching = FALSE,
            initComplete = JS(
              "function(settings, json) {",
              "$(this.api().table().container()).css({'font-size': '12px'});",
              "}"
            )
          ),
          rownames = FALSE
)
```

---

```{r, echo=TRUE}
# Fit a linear regression model
lm_model <- lm(Income ~ Education, data = d)
# Display the summary of the linear regression model
summary(lm_model)

```

---
### Interactions

Consider a regression:

$$\text{Duration}_i=\beta_0+\beta_1\text{Occupancy}_i+\beta_2\text{Male}_i+u_i $$
- Where Male is a for patient $i$ being male
- We assumed that occupancy has always the same effect, independent of your gender

--
- But what if occupancy matters more for men?
- In other words: one additional patient on urgent care increases duration by more if you are a men?
- Why? Maybe because when there is a lot of patients, doctors prioritize women (or men)

--
- We want allow the coefficient on occupancy to differ by gender. How?

---
### Interactions

- Run the regression: 

$$\text{Duration}_i=\beta_0+\beta_1\text{Occupancy}_i+\beta_2\text{Male}_i+\beta_3\text{Occupancy}_i*\text{Male}_i +u_i$$

--
- What's the coefficient on Occupancy when you are a woman $Male_i=0$?

$$\begin{align*}
& \text{Duration}_i=\beta_0+\beta_1\text{Occupancy}_i+\beta_2\text{Male}_i+\beta_3\text{Occupancy}_i*0 +u_i \\
& \text{Duration}_i=\beta_0+\beta_1\text{Occupancy}_i+\beta_2\text{Male}_i+u_i
\end{align*}$$


--

- What's the coefficient on Occupancy when you are a man $Male_i=1$?

$$\begin{align*}
& \text{Duration}_i=\beta_0+\beta_1\text{Occupancy}_i+\beta_2\text{Male}_i+\beta_3\text{Occupancy}_i*1 +u_i \\
& \text{Duration}_i=\beta_0+(\beta_1+\beta_3)\text{Occupancy}_i+\beta_2\text{Male}_i+u_i
\end{align*}$$

--

We can estimate $\beta_3$ and it will tell us by how much bigger is the coefficient on occupancy for men compared to the coefficient on occupancy for women. 

- $\beta_1$ is the coefficient for women
- $\beta_1+\beta_3$ is the coefficient for women
- $\beta_3$ is the difference in slopes, which we can test like other coefficients

---

```{r, echo=FALSE}
# Fit a linear regression model
lm_model <- lm(Duration ~ Occupancy*SEXO, data = Sample_urg[Sample_urg$SEXO!="NO ESPECIFICADO",])
# Display the summary of the linear regression model
summary(lm_model)

```

- One additional patients increases duration for women by 2.69 minutes
- One additional patients increases duration for men by  2.69+2.61=5.2 minutes

--
- What could be reasons for this?

---

### Interactions

- More generally, we can rewrite a regression:

$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3x_{i1}*x_{i2}+u_i$$
- As

$$y_i=\beta_0+(\beta_1+\beta_3x_{i2})x_{i1}+\beta_2x_{i2}++u_i$$
--
- $\beta_3$ answers the following question:
  - If I increase $x_{i2}$ by one, by how much the coefficient on $x_{i1}$ changes?


---

### Interactions

- Suppose you want to know who benefits the most from working from home. You collect survey data for each employee on the job satisfaction, whether they work in the office or from home, and the distance between the office and home

--

- Who do you think benefits most from working from home?

--
- How would you test this?

--

$$\text{Satisfaction}_i=\beta_0+\beta_1\text{WFH}_i+\beta_2\text{Distance}_i+\beta_3\text{WFH}_i*\text{Distance}_i +u_i$$


--
- What's the interpretation of $\beta_3$?

--

- By how much the effect of working from home on satisfaction changes when we increase distance by one unit (km)

--

- Which sign do you expect $\beta_3$ to have?

---

### Goodness of fit

- We can use again the R square to measure the goodness of fit.

$$\small R^2=1-\frac{\sum(y_i-\hat{y}_i)^2}{\sum(y_i-\bar{y}_i)^2}$$
--

- However, there is one problem with it.
  - Even if we add variables unrelated to $y$, the $R^2$ would typically still increase by a bit

--
  - Even if in population there is 0 relationship with this variable, our sample is small so we will  never get exactly 0 relationship

--
  - Sampling noise will make coefficient slightly positive or negative
  
--
  - So the increase in $R^2$ will reflect that noise in our sample 
  
--
  - The more coefficients we include, the higher $R^2$
  - We can adjust it, by accounting for the number of parameters used
  
--

$$\small R_{Adj}^2=1-\frac{\sum(y_i-\hat{y}_i)^2/(n-p)}{\sum(y_i-\bar{y}_i)^2/(n-1)}$$

--
- More parameters -> $\downarrow(n-p)\rightarrow\uparrow\sum(y_i-\hat{y}_i)^2/(n-p)\rightarrow\downarrow R_{Adj}^2$
- So it balances off the mechanical effect of higher $R^2$ due to more regressors
---

```{r, echo=FALSE}
# Fit a linear regression model

lm_model <- lm(Duration ~ Occupancy+EDAD, data = Sample_urg[Sample_urg$SEXO!="NO ESPECIFICADO",])
# Display the summary of the linear regression model
summary(lm_model)

```

---

```{r, echo=FALSE}
# Fit a linear regression model
Sample_urg$Random_var<- runif(nrow(Sample_urg), 1,10)
lm_model <- lm(Duration ~ Occupancy+EDAD+Random_var, data = Sample_urg[Sample_urg$SEXO!="NO ESPECIFICADO",])
# Display the summary of the linear regression model
summary(lm_model)

```
- Adding random variable increased $R^2$ but not $R^2_{Adj}$

---
layout: false
class: inverse, middle

# Statistical Properties of OLS


---

### Inference

- Let's add the assumption that errors are normally distributed:

$$ \mathbf{u} \sim N(0,\sigma I) $$
Which means that: 

$$y \sim N(X\beta,\sigma I) $$

- With inference we can:

  - Do hypothesis testing on single coefficients, ex: $H_0: \beta_2=0$
  - Find confidence intervals for a single coefficients
  - Do hypothesis testing on multiple coefficients: ex: $H_0: \beta_1=\beta_2$


---
### Test for a Single Coefficient

Under the above assumptions:

$$\hat{\beta}\sim N(\beta, \sigma\sqrt{(X'X)^{-1}})$$
And 

$$\hat{\beta_j}\sim N(\beta, \sigma\sqrt{(X'X)^{-1}_{j+1,j+1}})$$
--
Normalizing we get that:

$$\frac{\hat{\beta}_j-\beta_j}{s\sqrt{(X'X)^{-1}_{j+1,j+1}}} \sim t_{n-p}$$
- This test statistic has student t distribution with n-p degrees of freedom
  - Because the $\frac{s^2(n-p)}{\sigma^2} \sim \chi_{n-p}$
- Where p is the number of parameters (coefficients)
- $p=k+1$: k regressors and 1 intercept

---
### Test for a single coefficient

Suppose:
- $H_0: \beta_j=\beta_{j0}$
- $H_A: \beta_j \neq \beta_{j0}$

Then, we use test statistic:

$$t_{test}=\frac{\hat{\beta}_j-\beta_{j0}}{s\sqrt{(X'X)^{-1}_{j+1,j+1}}}$$
And we reject if $t_{test}>t_{\alpha/2,n-p}$ or $t_{test}<-t_{\alpha/2,n-p}$

Where $t_{\alpha/2,n-p}$ is $1-\alpha/2$ quantile of student t with n-p degrees of freedom
--

**NOTE:** This is a test for $\beta_j$ given all other regressors. It's not the same as the test statistic with only one regressor!
---
### Example

Suppose:
- $H_0: \beta_{Age}=0$
- $H_A: \beta_{Age} \neq 0$

```{r, echo=FALSE}
# Fit a linear regression model
lm_model <- lm(Duration ~ Occupancy+EDAD, data = Sample_urg)
# Display the summary of the linear regression model
summary(lm_model)

```


---
### Confidence Interval for a  Single Coefficient

We can also use this distribution to construct confidence intervals: 

An interval for $\beta_j$ with confidence level $1-\alpha$ is:

$$\begin{align*}
CI_{1-\alpha} & =\{\hat{\beta_j}-t_{\alpha/2,n-p}SE(\hat{\beta}_j),\hat{\beta_j}+t_{\alpha/2,n-p}SE(\hat{\beta}_j)\} \\
& =\{\hat{\beta_j}-t_{\alpha/2,n-p}s\sqrt{(X'X)^{-1}_{j+1,j+1}},\hat{\beta_j}+t_{\alpha/2,n-p}s\sqrt{(X'X)^{-1}_{j+1,j+1}}\}
\end{align*}$$


--

**Intepretation:**
- We are $1-\alpha$ % confident that the true parameter is within this CI
- If we take repeated samples, $1-\alpha$ %  of such constructed confidence intervals would contain true $\beta$

---

### Example:

For our age coefficient we had:
- $\hat{\beta}_{Age}=0.206$
- $SE(\hat{\beta})=0.067$
- Our $n=5000$ so we can use normal approximation

--

So 95% CI for $\beta_{Age}$ is:

$$\begin{align*}
CI_{1-\alpha} & =\{\hat{\beta_j}-t_{\alpha/2,n-p}SE(\hat{\beta}_j),\hat{\beta_j}+t_{\alpha/2,n-p}SE(\hat{\beta}_j)\} \\
& =\{0.206-1.96*0.067,0.206+1.96*0.067\} \\
& =\{0.075,0.337\} 
\end{align*}$$

--
- Note that the CI does not contain 0
- What does it imply for hypothesis testing with $H_0: \beta_{age}=0$?

---
### CI for mean response 

.pull-left[
Suppose that we want an average prediction for individuals with these characteristics:

$$\mathbf{x_0}=\begin{bmatrix}
1 \\
x_{01} \\
x_{02}  \\
\vdots \\
x_{0k} \\
\end{bmatrix}$$

Ex: What's average income ( $y$ ), for people who whave 12 years of education $x_{01}=12$ (2 other people are there) and are age 50 $x_{02}=50$ 


How accurate is our prediction?  $$\hat{y}_0=\mathbf{x_0}'\hat{\beta}$$ 

]
.pull-right[

The prediction is unbiased:

$E(\hat{y}_0)=\mathbf{x_0}'\beta$

and it's variance is:

$$\begin{align*} var(\hat{y}_0)& =var(\mathbf{x_0}'\hat{\beta}) \\
& =\mathbf{x_0}'var(\hat{\beta})\mathbf{x_0} \\ &=\sigma^2\mathbf{x_0}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x_0} \end{align*}$$

So it's distribution is:

$\hat{y}_0 \sim N(\mathbf{x_0}'\beta, \sqrt{\sigma^2\mathbf{x_0}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x_0}})$

Hence:

$CI_{1-\alpha}=\{\hat{y}_0\pm t_{n-2,\frac{\alpha}{2}}\sqrt{\sigma^2\mathbf{x_0}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x_0}}\}$
]


---
### Exanmple

What's the 95% CI for average wait time when there is 10 people at the Urgent Care $x_{occupancy}=10$ for a person who is of age 52  $x_{age}=52$ ?

- What do we need to answer this question?


  - $\hat{\beta}=\{\hat{\beta_0}, \hat{\beta}_{occupancy}, \hat{\beta}_{age}\}=\{23.236, 3.7, 0.2\}$

  - $\sqrt{\mathbf{x_0}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x_0}}=\sqrt{[1, 10, 52](\mathbf{X}'\mathbf{X})^{-1}[1, 10, 52]'}=0.021$

  - $\sigma=98.97$

--

- Prediction: $\hat{y_0}=23.236*1+3.7*10+0.2*52=70.636$

--
- Standard Deviation: $SE(\hat{y_0})=\sqrt{\sigma^2\mathbf{x_0}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x_0}}=2.07837$

--
$$CI_{95}=\{70.636 \pm 1.96*2.07837\} \approx \{67, 75\}$$
---
### Exanmple

Or simply in R: 

```{r, echo = TRUE}
lm_model <- lm(Duration ~ Occupancy+EDAD, data = Sample_urg)
new_data<- data.frame(Occupancy= c(10), EDAD=52)
predict(lm_model, newdata = new_data, interval = "confidence", level = 0.95, se.fit=TRUE)
```




---
### CI for new observation

** Reminder **: 
- When we look at average response, $u_i$ doesn't play a role (because on average errors are 0)
- When we look at a single observation, $u_i$ matters, so it increases the variance of prediction error

So variance is now the previous variance plus the variance of $u_i$

$$\begin{align*}var(y_0-\hat{y}_0)& =var(x_0\beta+u_i-x_0\hat{\beta})  \\ & =var(u_0)+var(x_0\hat{\beta}) \\ & =\sigma^2+\sigma^2\mathbf{x_0}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x_0}\end{align*}$$

So the confidence interval for a single observation is slightly wider:

$CI_{1-\alpha}=\{\hat{y}_0\pm t_{n-2,\frac{\alpha}{2}}\sqrt{\sigma^2(1+\mathbf{x_0}'(\mathbf{X}'\mathbf{X})^{-1}\mathbf{x_0})}\}$


We are less certain about predicting outcome for a single person, compared to average outcome among namy people. 

---
### Testing for the significance of the regression

- Does our model helps to explain any variation in $y_i$?

- $\small H_0: \beta_1=\beta_2=...\beta_k=0$
- $\small H_A: \beta_j \neq 0$ for at least one $j$
--

- It's the same procedure as before!
  - **Explained variation** should be large compared to **unexplained variation** if the model works
--

- We can again do the decomposition in SST, SSR, and SSE:
  - $SS_T$ is total sum of squares $\sum_i(y_i-\bar{y})^2$, .blue[n-1 DoF]
  - $SS_R$ is regression sum of squares $\sum_i(\hat{y_i}-\bar{y})^2$, .blue[k DoF]
  - $SS_E$ is residual error sum of squares $\sum_i(y_i-\hat{y_i})^2$, .blue[n-k-1 DoF]

```{r, results = "asis", echo = FALSE, message = FALSE}
library(knitr)

tex2markdown <- function(texstring) {
  writeLines(text = texstring,
             con = myfile <- tempfile(fileext = ".tex"))
  texfile <- pandoc(input = myfile, format = "html")
  cat(readLines(texfile), sep = "\n")
  unlink(c(myfile, texfile))
}

textable <- "
\\begin{table}[ht]
\\centering
\\begin{tabular}{lccc}
\\toprule
Source & Sum of Squares & Degrees of Freedom & DoF  \\\\
\\midrule
Regression & 13557462 & 2 & k  \\\\
Residual Error & 48947909 & 4995 & n-k-1  \\\\
Total & 62505371 & 4997 & n-1 \\\\
\\bottomrule
\\end{tabular}
\\end{table}

"

tex2markdown(textable)
```

---
### Testing for the significance of the regression

F-stat and its distribution under the null

$$F_{stat}=\frac{SSR/(k)}{SSE/(n-k-1)} \sim \underbrace{F_{k,n-k-1}}_{\text{Dist under } H_0}$$
--

Alternative way to think about it:

- $\small H_0: y=\beta_0+u$ restricted model
- $\small H_A: y=\beta_0+\beta_1x_{1}+\beta_2x_{2}+u$

If $H_A$ is true, restricted model should explain more of $y$

$$\small F_{stat}=\frac{SSR/(k)}{SSE/(n-k-1)}=\frac{\frac{\overbrace{SSR_{H_A}-SSR_{H_0}}^{\text{Extra Sum of Squares}}}{k-(k_0)}}{\frac{SSE_{H_A}}{n-k-1}}==\frac{\frac{\overbrace{SSE_{H_0}-SSE_{H_A}}^{\text{Extra Sum of Squares}}}{k-(k_0)}}{\frac{SSE_{H_A}}{n-k-1}}$$
--

- $\small SSR_{H_A}$ is the regression sum of square from unrestricted model with $\small k$ degrees of freedom (2)
- $\small SSR_{H_0}$ is the regression sum of squres from the restricted model wiht $\small k_0$ degrees of freedom (0) - it's the number of regressors in restricted model
- $\small SSE_{H_A}$ is the residual sum of squres in the unrestricted model

---
### Testing for the significance of the regression

```{r, echo = TRUE, message = FALSE}

linearHypothesis(lm_model, c("Occupancy=0", "EDAD=0"))

```

---
### Adding a coefficient 

- We can use the above logic to test how much more we can explain by including one more coeffcient


--
- Suppose we want to compare a regression model with only occupancy vs both occupancy and age

- $\small H_0: y=\beta_0+\beta_1Occupancy+u$ restricted model
- $\small H_A: y=\beta_0+\beta_1Occupancy+\beta_2Age+u$ unrestricted model

$$\small F_{2}=\frac{\frac{\overbrace{SSR_{H_A}-SSR_{H_0}}^{\text{Extra Sum of Squares}}}{k-(k_0)}}{\frac{SSE_{H_A}}{n-k-1}}=\frac{\frac{\overbrace{SSE_{H_0}-SSE_{H_A}}^{\text{Extra Sum of Squares}}}{k-(k_0)}}{\frac{SSE_{H_A}}{n-k-1}} \sim \underbrace{F_{k-k_0, n-k-1}}_{\text{Dist under } H_0}$$
- In our case $k=2$ and $k_0=1$, so the null distribution is $F_{1, n-3}

---
### Adding a coefficient 
```{r, echo = FALSE, message = FALSE}
lm_model <- lm(Duration ~Occupancy+EDAD, data = Sample_urg)
anova(lm_model)

```

- **Sequential testing:** 
- Occupancy - $F_1$ is the additional effect of including Occupancy to a model without any regressors
  - $\small H_0: y=\beta_0+u$ restricted model
  - $\small H_A: y=\beta_0+\beta_1Occupancy+u$ unrestricted model


- EDAD - $F_2$ is the additional effect of including Age once we already have Occupancy in the model
  - $\small H_0: y=\beta_0+\beta_1Occupancy+u$ restricted model
  - $\small H_A: y=\beta_0+\beta_1Occupancy+\beta_2Age+u$ unrestricted model



---
### Adding a coefficient 
- $F_k$ (last coef) is equivalent to $t_k^2$ in our full model
- But $F_1$ is not equivalent to $t_1^2$ in our full model

```{r, echo = FALSE, message = FALSE}
lm_model <- lm(Duration ~Occupancy+EDAD, data = Sample_urg)
summary_lm <- summary(lm_model)
coef_table <- summary_lm$coefficients
coef_table
anova(lm_model)

```

---
### Adding a coefficient 

- Why reordering variables changes $F_{stats}$? 

```{r, echo = FALSE, message = FALSE}
anova(lm_model)

lm_model <- lm(Duration ~EDAD+Occupancy, data = Sample_urg)
anova(lm_model)
```
--

- Because it changes which regressors we already have in the model

--
- Do squares always add up to the same thing?

---
### Testing multiple coefficients

Suppose we have a model with three predictors 

$$\small y=\beta_0+\beta_1Occupancy+\beta_2Age+\beta_3Male+u$$
We can test for a subset of predictors, for example if Age and Sex matter

- $H_0:\beta_2=\beta_3=0  \rightarrow y=\beta_0+\beta_1Occupancy+u$
- $H_A:\beta_2\neq 0$ or $\beta_3\neq0  \rightarrow y=\beta_0+\beta_1Occupancy+\beta_2Age+\beta_3Male+u$

$$\small F_{test}=\frac{\frac{\overbrace{SSR_{H_A}-SSR_{H_0}}^{\text{Extra Sum of Squares}}}{3-1}}{\frac{SSE_{H_A}}{n-3-1}}=\frac{\frac{\overbrace{SSE_{H_0}-SSE_{H_A}}^{\text{Extra Sum of Squares}}}{3-1}}{\frac{SSE_{H_A}}{n-3-1}} \sim F_{2, n-4}$$
---

```{r, echo = FALSE, message = FALSE}
lm_model <- lm(Duration ~Occupancy+EDAD+SEXO, data = Sample_urg[Sample_urg$SEXO!="SEXONO ESPECIFICADO",])
linearHypothesis(lm_model, c("EDAD=0", "SEXOMASCULINO=0"))
```


---

### Testing for multiple coefficients

A cool thing about the regression is that we can test relationships between the coefficients:

**For example**:
- Is the impact of additional year of experience the same as impact of additional year of work experience in a regression:

$$income_i=\beta_0+\beta_1\text{education}_i+\beta_2\text{experience}_i+u_i$$

- That corresponds to null hypothesis $H_0: \beta_1=\beta_2$ or $H_0: \beta_1-\beta_2=0$ 

**Another Example**:
- Suppose that employees can go through a sales training, and/or get a better office (these are binary variables). We want to evaluate impact of these measures on their sales:

$$Sales_i=\beta_0+\beta_1\text{training}_i+\beta_2\text{office}_i+u_i$$
- We wonder if giving an employee all three would increase sales by more than 100: 
$H_A: \beta_1+\beta_2>100$ 



---
### Relationships between coefficients 

Suppose we have a model:

$$y=\beta_0+\beta_1x_1+\beta_2x_2+...\beta_kx_k+u$$
--

- We want to test if the difference between impact of $x_1$ and $x_2$ is equal to c

**Hypothesis**
- $H_0: \beta_1-\beta_2=c$
- $H_A: \beta_1-\beta_2\neq c$

--
 - Special case: $c=0$ => testing equality $\beta_1=\beta_2$

--
**Test statistic and its distribution under the null**

$$T_{test}=\frac{\hat{\beta_1}-\hat{\beta_2}-c}{SE(\hat{\beta_1}-\hat{\beta_2})}=\frac{\hat{\beta_1}-\hat{\beta_2}-c}{\sqrt{var(\hat{\beta_1})+var(\hat{\beta_2})-2cov(\hat{\beta_1},\hat{\beta_2})}}\sim t_{n-k-1}$$
- Calculate p-value as $2P(t_{n-k-1}>|T_{test}|)$

---
### Relationships between coefficients

- In the same way we can test whether one coefficient is larger than another by some amount

**Hypothesis**
- $H_0: \beta_1-\beta_2=c$
- $H_A: \beta_1-\beta_2> c$
--

 - Special case: $c=0$ => testing inequality $\beta_1>\beta_2$

--

**Test statistic and its distribution under the null**

$$T_{test}=\frac{\hat{\beta_1}-\hat{\beta_2}-c}{SE(\hat{\beta_1}-\hat{\beta_2})}=\frac{\hat{\beta_1}-\hat{\beta_2}-c}{\sqrt{var(\hat{\beta_1})+var(\hat{\beta_2})-2cov(\hat{\beta_1},\hat{\beta_2})}}\sim t_{n-k-1}$$
--

- Calculate p-value as $P(t_{n-k-1}>T_{test})$
--

- If alternative is $H_A: \beta_1-\beta_2 < c$, then $P(t_{n-k-1}<T_{test})$


---
### Example

- Test if one more person at the hospital has larger effect than being one year older

```{r, echo = FALSE, message = FALSE}
m1=lm(formula = Duration ~ Occupancy + EDAD + SEXO, data = Sample_urg)
m1
vcov_matrix <- vcov(m1)
vcov_matrix
```

---
### Example

- Hypotheses:
  - $H_0: \beta_{O}=\beta_{A}$
  - $H_A: \beta_{O}>\beta_{A}$
--

- Calculate the test statistic

$$\small T_{test}=\frac{\beta_{O}-\beta_{A}}{\sqrt{var(\hat{\beta_O})+var(\hat{\beta_A})-2cov(\hat{\beta_O},\hat{\beta_A})}}=\frac{3.6803- 0.2047}{\sqrt{0.01+0.0045-2*(-0.00054)}}=27.84$$

--

- Calculate p-value

$P-value=P(t_{n-k-1}>T_{test})=P(t_{4994}>27.84) \approx0$


--

Conclusion

- we reject that impact of one more year is smaller or equal to the impact of one more person

---
### Sum of coefficients 

Suppose we have a model:

$$y=\beta_0+\beta_1x_1+\beta_2x_2+...\beta_kx_k+u$$
--

- We want to test if the sum of impact of $x_1$ and $x_2$ is equal to c

**Hypothesis**
- $H_0: \beta_1+\beta_2=c$
- $H_A: \beta_1+\beta_2\neq c$

--

**Test statistic and its distribution under the null**

$$T_{test}=\frac{\hat{\beta_1}+\hat{\beta_2}-c}{SE(\hat{\beta_1}+\hat{\beta_2})}=\frac{\hat{\beta_1}+\hat{\beta_2}-c}{\sqrt{var(\hat{\beta_1})+var(\hat{\beta_2})+2cov(\hat{\beta_1},\hat{\beta_2})}}\sim t_{n-k-1}$$
--

- Calculate p-value as $P(t_{n-k-1}>T_{test})$

- If $H_A: \beta_1+\beta_2 < c$, then $P(t_{n-k-1}<T_{test})$
- If $H_A: \beta_1+\beta_2 > c$, then $P(t_{n-k-1}>T_{test})$

---
### Example

- Test if the total impact of increasing occupancy by one person and being male is larger than 17


```{r, echo = FALSE, message = FALSE}
m1=lm(formula = Duration ~ Occupancy + EDAD + SEXO, data = Sample_urg)
m1
vcov_matrix <- vcov(m1)
vcov_matrix
```



---

### Standarized Coefficients

- Coefficients depend on the units of measurement of the $x$
- Since $x$ can have different units or magnitudes, we can't directly compare them


--

**Example:**

$$\text{ecobici trips}_i=\beta_0+\beta_1\text{temperature}_i+\beta_2\text{polution}_i+u_i$$

--

- It doesn't make sense to compare $\beta_1$ to $\beta_2$ to see what has bigger effect
- These variables have very different magnitudes
  -  Increasing temperature by one unit (1 degree celcius) is different than increasing polution by one unit (1 μg/m3)

--
- To make them directly comparable, we want to make them unitless (standarized)
- Does increasing temperature by .blue[one standard deviation] has the same effect as inreasing polution by .blue[one standard deviation]?


---
### Standarized coeffcients

Basically, we standardize all the variables and run the regression:

$$\frac{y_i-\bar{y}}{s_y}=\gamma_1\frac{x_{i1}-\bar{x}_{1}}{s_{x_1}}+\gamma_2\frac{x_{i2}-\bar{x}_2}{s_{x_2}}+...+\gamma_k\frac{x_{ik}-\bar{x}_k}{s_{x_k}}+u_i$$
So then $\gamma_k$ measures the impact of one standard deviation increase of $x_k$ on standard deviation in y


--

But there is a short cut to calculate these standard coefficients 

$$\gamma_k=\beta_k\frac{s_{x_k}}{s_y}$$


---

### Example

Urgent Care duration example:

- $s_y=111.82$
- $s_{Age}=20.82$
- $s_{Occupancy}=13.921$


--

We calculated that $\hat{\beta}_{Age}=0.206$ and $\hat{\beta}_{Occupancy}=3.703$

--

**Standardized coefficients**

$$\begin{align*} 
& \hat{\gamma}_{Age}  =\hat{\beta}_{Age}\frac{s_{Age}}{s_{y}}=0.206\frac{20.82}{111.82}=0.0383 \\
&\hat{\gamma}_{Occupancy} =\hat{\beta}_{Occupancy}\frac{s_{Occupancy}}{s_y}=3.703\frac{13.921}{111.82}=0.461
\end{align*}$$

--

- Changing age by one standard deviation increases duration by 3.8% of a standard deviation
- Changing occupancy one standard deviation increases duration by 46% of a standard deviation

---


