
```{r include=FALSE}
library(car)
library(wooldridge)
library(datarium)
simpleAnova <- function(object, ...) {

  # Compute anova table
  tab <- anova(object, ...)

  # Obtain number of predictors
  p <- nrow(tab) - 1

  # Add predictors row
  predictorsRow <- colSums(tab[1:p, 1:2])
  predictorsRow <- c(predictorsRow, predictorsRow[2] / predictorsRow[1])

  # F-quantities
  Fval <- predictorsRow[3] / tab[p + 1, 3]
  pval <- pf(Fval, df1 = p, df2 = tab$Df[p + 1], lower.tail = FALSE)
  predictorsRow <- c(predictorsRow, Fval, pval)

  # Simplified table
  tab <- rbind(predictorsRow, tab[p + 1, ])
  row.names(tab)[1] <- "Predictors"
  return(tab)

}



```


# Multiple Linear Regression

## Testing significance of the regression

In multiple linear regression, we model the relationship between a dependent variable \( y \) and multiple independent variables \( x_1, x_2, \dots, x_k \). Our goal is not only to predict \( y \) but also to understand how each predictor influences it.

In this chapter, we will carefully explore how to evaluate the significance of the overall model and how to test specific hypotheses about individual or groups of coefficients. We'll use the `marketing` dataset from the `datarium` package as an illustrative example. This dataset includes information on the budget allocated to advertising through YouTube, Facebook, and newspaper platforms, along with the corresponding sales generated.

```{r}

data(marketing)
head(marketing)
```

Businesses frequently invest in advertising across various media channels, and it’s essential to understand which channels effectively drive sales. This understanding helps optimize advertising budgets and maximize return on investment (ROI).


### Why Do We Need the F-Test?

The F-test in multiple regression helps us determine whether our model provides a significantly better fit than a model with no predictors. It is also called testing the significance of the regreession. 

Formally, the null hypothesis is:

\[
H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0
\]

In words, none of the independent variables have an effect on \( y \). That is, the model is useless. None of the variables can predict y.

Under \( H_0 \), the model reduces to:

\[
y = \beta_0 + u
\]

which is simply a horizontal line at the mean of \( y \). In this case, the regression is useless for explaining \( y \).

The alternative hypothesis is:

\[
H_A: \text{At least one } \beta_j \neq 0
\]

Rejecting \( H_0 \) suggests that at least one predictor significantly explains variation in \( y \). So the model is helpful in predicting y. 



### Intuition Behind the F-Test

If our model is useful, it should explain a large proportion of the total variability in \( y \).

To evaluate this, we decompose the total variability into:

- **Explained variation** (SSR): how much variability the regression explains
- **Unexplained variation** (SSE): how much variability remains unexplained (residuals)

Thus, we have:

\[
TSS = SSR + SSE
\]

where:

- **TSS**: Total Sum of Squares (total variation around the mean)
- **SSR**: Regression Sum of Squares (variation explained by the model)
- **SSE**: Error Sum of Squares (residual variation)



#### Degrees of Freedom

The decomposition and degrees of freedom (df) are summarized in the table:

| Component | Formula | Interpretation | Degrees of Freedom |
|--||--|--|
| TSS | \( \sum (y_i - \bar{y})^2 \) | Total variation around the mean | \( n - 1 \) |
| SSR | \( \sum (\hat{y}_i - \bar{y})^2 \) | Explained variation | \( k \) |
| SSE | \( \sum (y_i - \hat{y}_i)^2 \) | Unexplained (residual) variation | \( n - k - 1 \) |

Note:
- The degrees of freedom add up: \( (k) + (n - k - 1) = (n - 1) \).
- SSR + SSE = TSS.



### Understanding the F-Statistic

The F-statistic compares the mean explained variation to the mean unexplained variation:

\[
F = \frac{SSR / k}{SSE / (n - k - 1)}
\]

If the model is useless (all \( \beta_j = 0 \)), then SSR will be small, and the F-statistic will be close to 1.

If the model is useful (some \( \beta_j \neq 0 \)), SSR will be large compared to SSE, and the F-statistic will be significantly greater than 1.



### Intuitive Explanation of the F-Test

- If \( H_0 \) is true, \( F \) should be small.
- A large F-statistic provides evidence to reject \( H_0 \).

The p-value associated with the F-statistic is:

- \( p = P(F_{k, n-k-1} > F_{\text{observed}}) \)

where \( F_{k, n-k-1} \) denotes the F-distribution with \( k \) and \( n-k-1 \) degrees of freedom.

**Important**: 
- This is a **one-sided test**.
- We only reject \( H_0 \) if the F-statistic is sufficiently large (no need to double the p-value).

### Alternative Perspective: Extra Sum of Squares Approach

Another way to understand the F-test is to think in terms of comparing two models:

- **Restricted model** (under \( H_0 \)): only includes the intercept.
\[
H_0: y = \beta_0 + u
\]
- **Unrestricted model** (under \( H_A \)): includes all predictors.
\[
H_A: y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k + u
\]

If \( H_A \) is true, the unrestricted model should explain significantly more variation than the restricted model.

Thus, the F-statistic can also be written as:

\[
F = \frac{
\left( \text{SSE}_{\text{restricted}} - \text{SSE}_{\text{unrestricted}} \right) / (k - k_0)
}{
\text{SSE}_{\text{unrestricted}} / (n - k)
}
\]

where:

- \( \text{SSE}_{\text{restricted}} \): residual sum of squares from the restricted model (intercept only)
- \( \text{SSE}_{\text{unrestricted}} \): residual sum of squares from the full model
- \( k_0 \): number of predictors in the restricted model (typically 0)

Because the restricted model explains nothing (only the mean), \( SSR_{H_0} = 0 \), and the difference simplifies to \( SSR \) of the unrestricted model.

Thus, this "extra sum of squares" formulation is mathematically equivalent to the earlier F-formula based on explained and unexplained variation.


### Example with Marketing Data

Let's apply our detailed understanding of multiple regression analysis using the `marketing` dataset from the `datarium` package. Specifically, we want to examine how advertising expenditures on YouTube, Facebook, and newspapers influence product sales.

We will estimate the following regression equation:

\[
sales_i = \beta_0 + \beta_1 \, youtube_i + \beta_2 \, facebook_i + \beta_3 \, newspaper_i + u_i
\]

Each coefficient \( \beta \) measures the change in sales associated with each additional dollar invested in the respective advertising platform, holding the others constant.



#### Step-by-Step Regression in R

Here's how we perform this regression analysis using R:

```{r}
# Fit the multiple regression model
model <- lm(sales ~ youtube + facebook + newspaper, data = marketing)

# Display detailed summary
summary(model)
```



#### Interpreting the Model Output

Let's carefully interpret the output:

*Coefficients:**

1. **Intercept:** 3.5267 — the expected sales when there is no spending on any platform.
2. **YouTube coefficient:** 0.0458 — each additional dollar spent on YouTube advertising increases sales by approximately 0.0458 units, holding Facebook and newspaper spending constant.
3. **Facebook coefficient:** 0.1885 — each additional dollar spent on Facebook advertising increases sales by about 0.1885 units, ceteris paribus.
4. **Newspaper coefficient:** -0.0010 — spending on newspaper advertising shows a very small, statistically insignificant, negative effect on sales.

**Statistical significance:**

1. YouTube and Facebook coefficients are highly statistically significant (p-values < 0.001), indicating strong evidence that these platforms positively influence sales.
2. Newspaper advertising is not statistically significant (p-value ≈ 0.86), suggesting no effect.


#### ANOVA Table for Detailed Insights

The Analysis of Variance (ANOVA) table summarizes how much of the variability in sales is explained by the model.

The `anova()` function in R provides a decomposition of sum of squares for each predictor separately. To get the total explained variation (SSR), we add the sum of squares for YouTube, Facebook, and Newspaper.

```{r}
anova(model)
```

**Note:**  
- R reports the **Sum of Squares** for each predictor separately (e.g., YouTube, Facebook, Newspaper).  
- **Adding them together** gives the total explained sum of squares (SSR).

To obtain a simplified classic ANOVA table (model vs residuals), we can use a custom function like `simpleAnova(model)`

```{r}
simpleAnova(model)
```



#### Understanding the ANOVA Output

From the ANOVA table:

- **Sum of Squares Regression (SSR):** Add contributions from YouTube, Facebook, and Newspaper.  
  SSR ≈ 6,998.9
- **Sum of Squares Error (SSE):** Residuals ≈ 801.8
- **Degrees of Freedom:**
  - Regression df = 3 (number of predictors)
  - Residual df = 196 (total observations - predictors - 1)
- **Mean Squares:**
  - MSR = SSR / df_regression
  - MSE = SSE / df_residuals
- **F-statistic:**
  - Reported as 570.3
- **P-value:**
  - Less than 2.2e-16 (very strong evidence against \( H_0 \)).



#### Clearly Formulated Hypotheses

Let’s explicitly write the ANOVA F-test hypotheses:

\[
H_0: \beta_{\text{youtube}} = \beta_{\text{facebook}} = \beta_{\text{newspaper}} = 0
\]

\[
H_A: \text{At least one } \beta_j \neq 0
\]

Decision rule:
- **Reject \( H_0 \)** if the F-statistic is larger than the critical F-value (or if the p-value is smaller than 0.05).
- **Rejecting \( H_0 \)** confirms that advertising expenditures (at least on one platform) significantly affect sales.



#### Distribution of the F-Statistic Under the Null

Given that we have 200 observations, under \( H_0 \) (no advertising effect):

- Numerator degrees of freedom = 3 (predictors)
- Denominator degrees of freedom = \( n - k - 1 = 200 - 3 - 1 = 196 \)

At a 5% significance level, the critical value of the F-distribution \( F(3, 196) \) is approximately 2.65.



#### Calculating the F-Statistic Manually

Recall:

\[
F = \frac{SSR/k}{SSE/(n-k-1)}
\]

Substituting the numbers:

\[
F = \frac{6998.9/3}{801.8/196} = 570.3
\]

However, because in R’s calculation each contribution to SSR is added separately, and model fitting automatically adjusts for estimation, the reported F-statistic (570.3) corresponds correctly to the model summary and internal variance estimates.

(You can double-check by looking at Mean Squares: MSR / MSE.)



#### Conclusion

- The calculated F-statistic (570.3) is far larger than the critical value (2.65).
- The p-value is extremely small (0).
- Thus, we **strongly reject the null hypothesis**.
- Conclusion: Advertising spending on at least one platform significantly influences product sales.



## Relationships between coefficients 
Sometimes we are interested in testing the relationships between coefficients in the model. That allows us to compare effects of different variables or look at their sums. 

Consider a model: 
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k + u
\]


### Difference between coefficients

- We are interested in testing whether the **difference between the impact of** \( x_1 \) **and** \( x_2 \) **is equal to a constant** \( c \).

**Hypotheses:**
- \( H_0: \beta_1 - \beta_2 = c \)
- \( H_A: \beta_1 - \beta_2 \neq c \)



- A **special case** is when \( c = 0 \), meaning we are testing whether the coefficients are equal:
  
\[
H_0: \beta_1 - \beta=0
\]
Which is the same as:
\[
H_0: \beta_1 = \beta_2
\]



#### Test Statistic and Its Distribution Under the Null

The test statistic is:

\[
T_{\text{test}} = \frac{ \hat{\beta}_1 - \hat{\beta}_2 - c }{ SE(\hat{\beta}_1 - \hat{\beta}_2) }
\]

where

\[
SE(\hat{\beta}_1 - \hat{\beta}_2) = \sqrt{ \text{Var}(\hat{\beta}_1) + \text{Var}(\hat{\beta}_2) - 2\text{Cov}(\hat{\beta}_1, \hat{\beta}_2) }
\]

Under \( H_0 \), the test statistic follows a t-distribution:

\[
T_{\text{test}} \sim t_{n-k-1}
\]

where:
- \( n \) is the number of observations
- \( k \) is the number of predictors (excluding intercept)

We calculate the p-value as:

\[
\text{p-value} = 2P(t_{n-k-1} > |T_{\text{test}}|)
\]




### Testing Inequalities Between Coefficients

Similarly, we can test whether **one coefficient is greater than** another by a given amount.

**Hypotheses:**
- \( H_0: \beta_1 - \beta_2 = c \)
- \( H_A: \beta_1 - \beta_2 > c \)



- Special case when \( c = 0 \): testing whether \( \beta_1 > \beta_2 \).



#### Test Statistic and p-Value

The test statistic remains:

\[
T_{\text{test}} = \frac{ \hat{\beta}_1 - \hat{\beta}_2 - c }{ SE(\hat{\beta}_1 - \hat{\beta}_2) }
\]

and under \( H_0 \):

\[
T_{\text{test}} \sim t_{n-k-1}
\]

The p-value depends on the direction of the alternative hypothesis:

- For \( H_A: \beta_1 - \beta_2 > c \):
  
\[
\text{p-value} = P(t_{n-k-1} > T_{\text{test}})
\]

- For \( H_A: \beta_1 - \beta_2 < c \):

\[
\text{p-value} = P(t_{n-k-1} < T_{\text{test}})
\]



### Testing the sum of the coefficients: 


- We are interested in testing whether the **sum of the impacts of** \( x_1 \) **and** \( x_2 \) **is equal to a constant** \( c \).

**Hypotheses:**
- \( H_0: \beta_1 + \beta_2 = c \)
- \( H_A: \beta_1 + \beta_2 \neq c \)



- Helps to answer
  - Is combined effect bigger than some number?
  - Does increasing both at the same time has positive or negative effect (if coefficients have opposite signs)?
  
\[
H_0: \beta_1 + \beta=0
\]

vs 

\[
H_A: \beta_1 + \beta \neq 0
\]

or for one sided tests:

\[
H_A: \beta_1 + \beta >0
\]

or 

\[
H_A: \beta_1 + \beta <0
\]

#### Test Statistic and Its Distribution Under the Null

The test statistic is:

\[
T_{\text{test}} = \frac{ \hat{\beta}_1 + \hat{\beta}_2 - c }{ SE(\hat{\beta}_1 + \hat{\beta}_2) }
\]

where (sings on covariance changes):

\[
SE(\hat{\beta}_1 + \hat{\beta}_2) = \sqrt{ \text{Var}(\hat{\beta}_1) + \text{Var}(\hat{\beta}_2) + 2\text{Cov}(\hat{\beta}_1, \hat{\beta}_2) }
\]

Under \( H_0 \), the test statistic follows a t-distribution:

\[
T_{\text{test}} \sim t_{n-k-1}
\]

where:
- \( n \) is the number of observations
- \( k \) is the number of predictors (excluding intercept)

We calculate the p-value  for two sided test as:

\[
\text{p-value} = 2P(t_{n-k-1} > |T_{\text{test}}|)
\]

And for one sided test we don't mulitply by 2.



### Application Example: Testing Equality of Facebook and Newspaper Effects

Suppose we want to test whether the effect of Facebook advertising is **equal to** the effect of Newspaper advertising.

As a reminder:

| Coefficient  | Estimate | Std. Error |
|--|-||
| (Intercept)  | 3.526667 | 0.374290    |
| YouTube      | 0.045765 | 0.001395    |
| Facebook     | 0.188530 | 0.008611    |
| Newspaper    | -0.001037 | 0.005871   |

Formally:

\[
H_0: \beta_{\text{facebook}} = \beta_{\text{newspaper}}
\]

This is equivalent to testing:

\[
H_0: \beta_{\text{facebook}} - \beta_{\text{newspaper}} = 0
\]

against the alternative:

\[
H_A: \beta_{\text{facebook}} - \beta_{\text{newspaper}} \neq 0
\]



#### Manual Calculation: Step-by-Step

We can calculate the test manually. Extract the variance-covariance matrix of estimated coefficients:

```{r}
vcov(model)
```


- \( \text{Var}(\hat{\beta}_{\text{facebook}}) = 7.415335 \times 10^{-5} \)
- \( \text{Var}(\hat{\beta}_{\text{newspaper}}) = 3.446875 \times 10^{-5} \)
- \( \text{Cov}(\hat{\beta}_{\text{facebook}}, \hat{\beta}_{\text{newspaper}}) = -1.780062 \times 10^{-5} \)

_Remember_: In the variance-covariance matrix, the first row/column corresponds to the intercept!



#### Step 1: Calculate the Standard Error

We use the formula:

\[
SE(\hat{\beta}_{\text{facebook}} - \hat{\beta}_{\text{newspaper}})
= \sqrt{ \text{Var}(\hat{\beta}_{\text{facebook}}) + \text{Var}(\hat{\beta}_{\text{newspaper}}) - 2\text{Cov}(\hat{\beta}_{\text{facebook}}, \hat{\beta}_{\text{newspaper}}) }
\]

Substituting values:

\[
SE = \sqrt{ (7.415335 \times 10^{-5}) + (3.446875 \times 10^{-5}) - 2(-1.780062 \times 10^{-5}) }
\]
\[
SE = \sqrt{ (7.415335 + 3.446875 - 2 \times -1.780062) \times 10^{-5} }
\]
\[
SE = \sqrt{ 0.0001442233 }
\]
\[
SE \approx 0.0120093
\]

Thus:

\[
SE(\hat{\beta}_{\text{facebook}} - \hat{\beta}_{\text{newspaper}}) \approx 0.0120093
\]



#### Step 2: Calculate the Test Statistic

The formula for the test statistic is:

\[
T_{\text{test}} = \frac{ \hat{\beta}_{\text{facebook}} - \hat{\beta}_{\text{newspaper}} - 0 }{ SE(\hat{\beta}_{\text{facebook}} - \hat{\beta}_{\text{newspaper}}) }
\]

Substituting in the estimates:

\[
T_{\text{test}} = \frac{ 0.188530 - (-0.001037) }{ 0.0120093 }
\]
\[
T_{\text{test}} = \frac{ 0.189567 }{ 0.0120093 }
\]
\[
T_{\text{test}} \approx 15.78
\]



#### Step 3: Find the p-value

Under \( H_0 \), the test statistic follows a \( t \)-distribution with \( n - k - 1 \) degrees of freedom.

In this case:
- \( n = 200 \) observations
- \( k = 3 \) predictors (YouTube, Facebook, Newspaper)

Thus:

\[
\text{Degrees of Freedom} = 200 - 3 - 1 = 196
\]

The p-value for a two-sided test is:

\[
\text{p-value} = 2P(t_{196} > 15.78)
\]

Given that 15.78 is extremely large, the p-value is effectively close to zero (\( < 0.0001 \)).

Thus, we **strongly reject** the null hypothesis that the effect of Facebook equals the effect of Newspaper.



### Additional example: Comparing 4 Dollars in YouTube to 1 Dollar in Facebook

We want to test whether **spending 4 dollars on YouTube has the same effect as spending 1 dollar on Facebook**.

In other words, we want to test:

\[
H_0: 4\beta_{\text{youtube}} = \beta_{\text{facebook}}
\]

This can be rearranged into the standard form:

\[
H_0: 4\beta_{\text{youtube}} - \beta_{\text{facebook}} = 0
\]

**Alternative Hypothesis:**

\[
H_A: 4\beta_{\text{youtube}} - \beta_{\text{facebook}} \neq 0
\]



#### Step 1: Calculate the Estimate of the Linear Combination

From the regression output:

- \( \hat{\beta}_{\text{youtube}} = 0.045765 \)
- \( \hat{\beta}_{\text{facebook}} = 0.188530 \)

Thus:

\[
4\hat{\beta}_{\text{youtube}} - \hat{\beta}_{\text{facebook}} = 4(0.045765) - 0.188530
\]
\[
= 0.18306 - 0.188530 = -0.00547
\]



#### Step 2: Calculate the Standard Error

The variance of the linear combination \( 4\hat{\beta}_{\text{youtube}} - \hat{\beta}_{\text{facebook}} \) is:

\[
\text{Var}(4\hat{\beta}_{\text{youtube}} - \hat{\beta}_{\text{facebook}}) = 4^2 \times \text{Var}(\hat{\beta}_{\text{youtube}}) + (-1)^2 \times \text{Var}(\hat{\beta}_{\text{facebook}}) + 2 \times 4 \times (-1) \times \text{Cov}(\hat{\beta}_{\text{youtube}}, \hat{\beta}_{\text{facebook}})
\]

Substituting the given values from above variance covariance matrix:

\[
= 16 \times 1.945737\times 10^{-6} + 1 \times 7.415335\times 10^{-5} + 2 \times 4 \times (-1) \times (-4.470395\times 10^{-7})
\]
\[
= 16 \times 0.000001945737 + 0.00007415335 + 2 \times 4 \times 0.0000004470395
\]
\[
= 0.000031131792 + 0.00007415335 + 0.000003576316
\]
\[
= 0.00010886146
\]

Thus:

\[
SE = \sqrt{0.00010886146} \approx 0.01043
\]



#### Step 3: Calculate the Test Statistic

\[
T_{\text{test}} = \frac{-0.00547 - 0}{0.01043} \approx -0.524
\]



#### Step 4: Find the p-value

- Degrees of freedom = \( 200 - 3 - 1 = 196 \)
- The p-value for a two-sided test is:

\[
\text{p-value} = 2P(t_{196} > 0.524)=0.7
\]

Since \( T_{\text{test}} \) is small in absolute value, the p-value will be large (>0.5).

**Conclusion:**  
We **fail to reject** \( H_0 \). There is no evidence that the effect of 4 dollars in YouTube is different from 1 dollar in Facebook.




### Example 2: Testing if the Combined Effect of YouTube and Facebook Exceeds 0.22

Now, we want to test whether **the sum of the effects of 1 dollar in YouTube and 1 dollar in Facebook** is **greater than** 0.22.

Formally:

\[
H_0: \beta_{\text{youtube}} + \beta_{\text{facebook}} = 0.22
\]
\[
H_A: \beta_{\text{youtube}} + \beta_{\text{facebook}} > 0.22
\]



#### Step 1: Calculate the Estimate of the Linear Combination

\[
\hat{\beta}_{\text{youtube}} + \hat{\beta}_{\text{facebook}} = 0.045765 + 0.188530 = 0.234295
\]



#### Step 2: Calculate the Standard Error

The variance of the linear combination \( \hat{\beta}_{\text{youtube}} + \hat{\beta}_{\text{facebook}} \) is:

\[
\text{Var}(\hat{\beta}_{\text{youtube}} + \hat{\beta}_{\text{facebook}}) = \text{Var}(\hat{\beta}_{\text{youtube}}) + \text{Var}(\hat{\beta}_{\text{facebook}}) + 2\text{Cov}(\hat{\beta}_{\text{youtube}}, \hat{\beta}_{\text{facebook}})
\]

Substituting values:

\[
= 1.945737\times 10^{-6} + 7.415335\times 10^{-5} + 2 \times (-4.470395\times 10^{-7})
\]
\[
= 0.000001945737 + 0.00007415335 - 0.000000894079
\]
\[
= 0.000075205008
\]

Thus:

\[
SE = \sqrt{0.000075205008} \approx 0.008673
\]



#### Step 3: Calculate the Test Statistic

\[
T_{\text{test}} = \frac{0.234295 - 0.22}{0.008673} = \frac{0.014295}{0.008673} \approx 1.648
\]



#### Step 4: Find the p-value

Since this is a **one-sided test** (greater than), we compute:

\[
\text{p-value} = P(t_{196} > 1.648)=0.0504
\]

Consulting the t-distribution, the p-value is approximately 0.05.

**Conclusion:**
- The p-value is slightly above 0.05.
- Thus, there is no evidence to reject \( H_0 \) at the 5% level, but you could reject at 10\%
- **Interpretation**: There is weak evidence that the combined effect exceeds 0.22. Or you could say there is no strong evidence to reject. 



## Standarized Coefficients

### Why Standardized Coefficients?

Sometimes, when you run a regression, the variables you include have very different units or vary at very different scales. For instance, consider a company trying to predict **employee turnover** based on the following predictors:

- **Salary** in **dollars**,
- **Tenure - Years at the company**,
- **Satisfaction score** on a **1–5 scale**.

You might estimate:

\[
\text{Turnover}_i = \alpha + \beta_1 (\text{Salary}_i) + \beta_2 (\text{Tenure}_i) + \beta_3 (\text{Satisfaction Score}_i) + \varepsilon_i
\]

where:
- Salary is in dollars (e.g., 55,000),
- Tenure is in years (e.g., 3.5),
- Satisfaction Score is on a small discrete scale (1 to 5).

Comparing coefficients across variables that are in such different units makes interpretation difficult. A one-dollar increase in salary is not easy to compare to a one-point increase in satisfaction or one more year in a company, because they have different units. These comparisons are technically possible. But they don't necessarily help to understand which of the variables are most important in determining the turnover. To see what's important, we need to see how much variables change and what units they have, not only their coefficients. 

### Why Variability Matters

When we think about the "importance" of a variable in explaining an outcome, we need to consider **two things**:

- The **beta coefficient**: how much \( Y \) changes when the predictor increases by one unit.
- The **variability** of the predictor: how much the predictor itself varies. Does it usually increase by one unit? by more or by less?

Even if a variable has a large effect for individuals (large beta), if it hardly varies in the population, it will explain very little of the total variation in the outcome.


Consider a simple model predicting **expected age at death**:

\[
\text{Expected Age at Death}_i = 70  -30 (\text{Genetic Disease}_i) - 0.5 (\text{Smoking Intensity}_i) + 0.01 (\text{Income}_i) + \varepsilon_i
\]

where:

- **Genetic Disease** is a binary indicator (1 if the individual carries a rare genetic mutation, 0 otherwise),
- **Smoking Intensity** is the number of cigarettes smoked per day,
- **Income** is the annual income in dollars.

Suppose the following facts:

- **Rare Genetic Disease**:  
  - Increases mortality risk dramatically (beta is large, e.g., reduces expected age at death by 40 years).
  - But it occurs in only with very small probability.  
  - Standard deviation is very close to zero because almost no one carries the mutation.

- **Smoking Intensity**:  
  - Each additional cigarette smoked per day reduces life expectancy slightly (e.g., beta = \(-0.5\) years).
  - But people differ widely in how much they smoke — some smoke 0, others smoke 10, 20, or more cigarettes per day.
  - Standard deviation is large.

- **Income**:  
  - Each extra $1 of income increases expected life by only a tiny amount (beta is small, e.g., \(0.01\) years).
  - But income varies a lot across people, from very low to very high, by tens of thousands of dollars.
  
While the rare genetic disease has a **huge individual effect**, it affects so few people that it contributes **very little** to explaining differences in life expectancy **across the population**.

In contrast, smoking has a **small per-unit effect**, but because smoking behavior **varies a lot** among individuals, it explains **much more** of the variability in expected age at death.

Similarly, **income** has a very small beta, but because income **varies widely**, it can still be an important predictor overall.

When comparing importance of predictors, we care about **both**:

- How much an additional unit matters (beta),
- How much the predictor varies across individuals (standard deviation).

Standardizing variables solves this: it puts everything on the **same scale** — standard deviation units — so we can **compare** the relative importance of predictors fairly.

Instead of looking at "effect per $1" or "effect per cigarette," we look at "effect per one standard deviation change," making the different predictors directly comparable.

### How Do We Standardize?

To put all variables on a common, comparable scale, we **standardize** them.

Standardizing means expressing variables in terms of their **standard deviation**:

\[
X_j^{*} = \frac{X_j - \text{Mean}(X_j)}{\text{SD}(X_j)}
\]

where:

- \(X_j^{*}\) is the **standardized version** of variable \(X_j\),
- \(\text{Mean}(X_j)\) is the **mean** of \(X_j\),
- \(\text{SD}(X_j)\) is the **standard deviation** of \(X_j\).


In a **standardized regression**, the coefficients tell us:

- How many standard deviations the outcome changes when the predictor increases by one standard deviation.

This transformation allows us to compare predictors fairly, even when they originally have very different units or ranges.


### Regression with Standardized Variables

When we standardize all variables — both the predictors and the outcome — the regression model becomes:

\[
Y^{*} = \gamma_1 X_1^{*} + \gamma_2 X_2^{*} + \cdots + \gamma_N X_N^{*} + \varepsilon
\]

Notice that there is **no intercept** in a standardized regression.  
**Why?**  
Because when all variables (including the outcome) are standardized to have **mean zero**, the intercept automatically becomes zero: the best-fitting line must pass through the origin. (Think to the formulat for intercept in simple linear regression with one variable). R will sometimes show intercept in standardized regression, but it is a rounding mistake and it should be close to 0. 

Each \(\gamma_j\) tells us:

> How many standard deviations the outcome changes when the predictor increases by one standard deviation, holding other variables constant.

This makes coefficients directly comparable, even when variables originally had different units or variability.

### Example: Predicting Birth Weight

We use the **`bwght`** dataset from Wooldridge's textbook. It contains information about mothers and their children, including:

- **bwght** — birth weight (ounces) of the child,
- **motheduc** — mother's years of education,
- **faminc** — family income (thousands of dollars),
- **cigs** — average number of cigarettes smoked per day during pregnancy.
- **white** — is mother white

Suppose we want to predict child birth weight based on these characteristics:

\[
\text{bwght}_i = \alpha + \beta_1 (\text{motheduc}_i) + \beta_2 (\text{faminc}_i) + \beta_3 (\text{cigs}_i) + \varepsilon_i
\]

```{r}
# Load the data
data("bwght")

# Raw regression
model_raw <- lm(bwght ~ motheduc + faminc + cigs+white, data = bwght)
summary(model_raw)
```
Clearly, the biggest coefficient comes on white. That is, white mothers generally have children with higher birth weight. Then we also see importance of the cigs, which shows that smoking one more cigarette a day is associated witha  weight decrease of the child by 0.475 ounce (one ounce is 28.3 grams). But race and smoking come in very different units. 

Next, we standardize both the outcome and the predictors and run the standardized model:

```{r}
# Standardizing variables manually
bwght$bwght_std <- scale(bwght$bwght)
bwght$motheduc_std <- scale(bwght$motheduc)
bwght$faminc_std <- scale(bwght$faminc)
bwght$cigs_std <- scale(bwght$cigs)
bwght$white_std <- scale(bwght$white)

# Regression with standardized variables
model_std <- lm(bwght_std ~ motheduc_std + faminc_std + cigs_std+white_std, data = bwght)
summary(model_std)
```
The coefficients changed (and statistical significance didn't). Standardizing affects only coefficient sizes. The comparison shows us that smoking is actually more important in explaining the weight differences between children. One standard deviation increase in number of cigarettes smoked per day is associated with a decrease in birth weight of 0.14 standard deviations. The coefficient on white is now smaller (in absolute terms). This is because cigarettes can change by 5 or more, why the indicator for race can maximally change by one. So although it had bigger effect per one unit change, the total importance is smaller because that variable doesn't vary as much.

**Note:** When comparing importance of the coefficients, we mostly care about size in absolute terms.  

### Shortcut to calculate standardized coefficients.

Do we always have to run a new regression to calculate standardized coefficients? No! All you need is your regular regression and standard deviations of the variables. 

We can calculate standardized coefficients directly from the original regression using the following formula:

\[
\beta_j^* = \beta_j \times \frac{\text{SD}(X_j)}{\text{SD}(Y)}
\]

where:

- \(\beta_j\) is the **raw (unstandardized) coefficient** from the original regression,
- \(\text{SD}(X_j)\) is the **standard deviation** of predictor \(X_j\),
- \(\text{SD}(Y)\) is the **standard deviation** of the outcome variable \(Y\).\

This formula rescales each coefficient based on how much its corresponding predictor and the outcome vary in the data.  
It produces the **same result** as manually standardizing all variables before running the regression.

In our example, the original coefficient on **cigarettes** was:

\[
\beta_{\text{cigs}} = -0.47552
\]

The standard deviation of **cigarettes** was:

\[
\text{SD}(\text{cigs}) = 5.972688
\]

The standard deviation of **birth weight** was:

\[
\text{SD}(\text{bwght}) = 20.35396
\]

Plugging these values into the formula:

\[
\beta_{\text{cigs}}^* = -0.47552 \times \frac{5.972688}{20.35396}
\]

Simplifying:

\[
\beta_{\text{cigs}}^* \approx -0.139536
\]

We got the same standarized coefficient as from the standarized regression. 

