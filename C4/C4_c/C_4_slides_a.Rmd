---
title: 'Class 4c: Simple OLS:  ANOVA and F-test'
author: "Business Forecasting"
output:
  xaringan::moon_reader:
    self_contained: true
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: true
      
---   
<style type="text/css">
.remark-slide-content {
    font-size: 20px;
}


</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dpi=300)
library(shiny)
library(ggplot2)
library(forecast)
library(plotly)
library(dplyr)
library(igraph)
library(reshape)
library(spData)
library(leaflet)
library(readr)
library(LaplacesDemon)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(hrbrthemes)
library(gridExtra)
library(cowplot)
library(viridis)
library(gapminder)
library(knitr)
library(kableExtra)
library(DT)

```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(base_color = "#43418A", 
colors = c(
  red = "#f34213",
  purple = "#3e2f5b",
  orange = "#ff8811",
  green = "#136f63",
  blue = "#1E90FF",
  white = "#FFFFFF"
))
```


```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(dplyr)
library(MASS) # for truehist function
load("Daily_CMX_means2.Rda")

```
---
## Roadmap


### This class
- Testing assumptions behind residuals
  - Linearity
  - Constant Variance
  - Uncorrelated Residuals
  - Normality
  
---
### ANOVA

**ANOVA** stands for the .blue[AN]alysis .blue[O]f .blue[V]ariance

- We only look at it in the context of the regression
- It helps us to determine wheather our regression is helpful
  - It tests whether our regression model can explain variation in y
  
---
### ANOVA

How do we measure explained variation?

$$\small \underbrace{y_i-\bar{y}}_{\substack{Total \\ deviation}}=\underbrace{(\hat{y_i}-\bar{y})}_{\substack{Explained \\ deviation}}+\underbrace{(y_i-\hat{y_i})}_{\substack{Unexplained \\ deviation}}$$
where $\small \hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_i$


```{r, warning=FALSE, fig.height=3, out.width='100%', message=FALSE}
# Simulate data with increasing variance of residuals
set.seed(123)
x <- seq(1, 200, by = 1)  # Predictor variable
y <- 1.5 * x + rnorm(200, mean = 0, sd = 30)  # Response variable with increasing variance

# Fit a linear regression model
model <- lm(y ~ x)

# Create a data frame with the data
data_df <- data.frame(
  x = x,
  y = y
)

# Create a data frame with predicted values from the model
predicted_df <- data.frame(
  x = x,
  y_pred = predict(model)
)

# Calculate deviations for x = 150
x_value <- 72
y_mean <- mean(y)
y_observed <- data_df$y[data_df$x == x_value]
y_predicted <- predicted_df$y_pred[predicted_df$x == x_value]

# Add these deviations as a new data frame
deviations_df <- data.frame(
  x = c(x_value, x_value, x_value),
  y_start = c(y_mean, y_mean, y_predicted),
  y_end = c(y_predicted, y_observed, y_observed)
)

# Create a scatterplot of the data points
# Add the regression line, dashed lines to show deviations, three vertical segments for our x=150, and a horizontal line for mean y
ggplot(data_df, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x, color = "blue") +
  geom_segment(data = predicted_df, aes(xend = x, yend = y_pred), linetype = "dashed", color = "gray", alpha=0.7) +
  geom_segment(data = deviations_df, aes(x = x, xend = x, y = y_start, yend = y_end), color = c("green", "red", "purple"), size = 1.2) +
  geom_hline(yintercept = y_mean, linetype = "dotdash", color = "orange", size =1.5) +
  labs(
    x = "Predictor Variable (x)",
    y = "Response Variable (y)"
  ) +theme_xaringan()

```

---
### ANOVA
Let's move from a single deviation to sum of squared deviations:

From here:
$$y_i-\bar{y}=(\hat{y_i}-\bar{y})+(y_i-\hat{y_i})$$
To here:

$$\sum_i(y_i-\bar{y})^2=\sum_i(\hat{y_i}-\bar{y})^2+\sum_i(y_i-\hat{y_i})^2$$

**Decomposition of variance**

$$SS_T=SS_R+SS_E$$
- $SS_T$ is total sum of squares $\sum_i(y_i-\bar{y})^2$, .blue[1 DoF]
- $SS_R$ is regression sum of squares $\sum_i(\hat{y_i}-\bar{y})^2=\hat{\beta}_1S_{XY}$, .blue[n-2 DoF]
- $SS_E$ is residual error sum of squares $\sum_i(y_i-\hat{y_i})^2$, .blue[n-1 DoF]

---
### ANOVA

- Good model explains a lot of variation in $y$
- Bad model explains little variation in $y$

```{r, warning=FALSE, fig.height=4, out.width='100%', message=FALSE}
# Simulate data with increasing variance of residuals
library(ggplot2)

# Set seed for reproducibility
set.seed(123)

# Predictor variable
x <- seq(-100, 100, by = 1)  

# 1. Dataset with high model fit
y1 <- 10+2 * x + rnorm(200, mean = 0, sd = 20)  # Low variability around the regression line

# 2. Dataset with low model fit
y2 <- 10+0.5 * x + rnorm(200, mean = 0, sd = 100)  # High variability around the regression line

# Fit linear regression models
model1 <- lm(y1 ~ x)
model2 <- lm(y2 ~ x)

# Create data frames
data_df1 <- data.frame(x = x, y = y1)
data_df2 <- data.frame(x = x, y = y2)

# Predicted values
predicted_df1 <- data.frame(x = x, y_pred = predict(model1))
predicted_df2 <- data.frame(x = x, y_pred = predict(model2))

# Calculate deviations for x = 150
x_value <- 50

y_mean1 <- mean(y1)
y_mean2 <- mean(y2)

y_observed1 <- data_df1$y[data_df1$x == x_value]
y_observed2 <- data_df2$y[data_df2$x == x_value]

y_predicted1 <- predicted_df1$y_pred[predicted_df1$x == x_value]
y_predicted2 <- predicted_df2$y_pred[predicted_df2$x == x_value]

deviations_df1 <- data.frame(
  x = c(x_value, x_value),
  y_start = c(y_mean1, y_predicted1),
  y_end = c(y_predicted1, y_observed1)
)

deviations_df2 <- data.frame(
  x = c(x_value, x_value),
  y_start = c(y_mean2, y_predicted2),
  y_end = c(y_predicted2, y_observed2)
)

# Plotting functions
plot_deviations <- function(data_df, predicted_df, deviations_df, y_mean, title, line_color) {
  ggplot(data_df, aes(x = x, y = y)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, formula = y ~ x, color = line_color) +
    geom_hline(yintercept = y_mean, linetype = "dotdash", color = "orange") +
    geom_segment(data = deviations_df, aes(x = x, xend = x, y = y_start, yend = y_end), color = c("green", "red"), size = 1.2) +
    labs(title = title, x = "Predictor Variable (x)", y = "Response Variable (y)") +
    theme_minimal()+ylim(-300,300)
}

# Generate plots
plot1 <- plot_deviations(data_df1, predicted_df1, deviations_df1, y_mean1, "High Model Fit", "blue")
plot2 <- plot_deviations(data_df2, predicted_df2, deviations_df2, y_mean2, "Low Model Fit", "red")


grid.arrange(plot1, plot2, ncol=2)

```

---
We usually write them in a table. Here is how it looks like for our ecobici data:
```{r, results = "asis", echo = FALSE, message = FALSE}
library(knitr)

tex2markdown <- function(texstring) {
  writeLines(text = texstring,
             con = myfile <- tempfile(fileext = ".tex"))
  texfile <- pandoc(input = myfile, format = "html")
  cat(readLines(texfile), sep = "\n")
  unlink(c(myfile, texfile))
}

textable <- "
\\begin{table}[ht]
\\centering
\\begin{tabular}{lccc}
\\toprule
Source & Sum of Squares & Degrees of Freedom & DoF  \\\\
\\midrule
Regression & 2117100000 & 1 & 1  \\\\
Residual Error & 21895000000 & 779 & n-2  \\\\
Total & 24012100000 & 780 & n-1 \\\\
\\bottomrule
\\end{tabular}
\\end{table}

"

tex2markdown(textable)
```

Or in R:
```{r, warning=FALSE, fig.height=4, out.width='100%', message=FALSE, echo=TRUE}
model=lm(Trips ~ TMP, data = Data_BP)
anova(model)

```

---
### ANOVA

- But how do we use it as a formal test?

--
- If our model (our predictor) is not helpful in explaining the $y$, then likely $\beta_1=0$

--
- We can use the sum of squares to test:
  - $H_0:\beta_1=0$
  - $H_A:\beta_1 \neq 0$

--
- **Test statistic** is:

$$F_{test}=\frac{SS_R/df_R}{SS_E/df_E}$$
Where 
- $SS_R$ has 1 degree of freedom $df_R=1$
- $SS_E$ has n-2 degree of freedom $df_E=n-2$

--
- Under the null:

$$F_{test} \sim F_{1,n-2}$$
And we reject if $F_{test}>F_{1-\alpha,1,n-2}$ (when $F_{test}$ is large)

---
### ANOVA
Whether $\beta_1=0$ or $\beta_1 \neq 0$:

$$E(\frac{SS_E}{df_E})=E(\frac{\sum e^2}{n-2})=\sigma^2$$
And it's distributed as:
$$\frac{SS_E}{σ^2} \sim \chi_{n-2}$$

Only if null is true ( $\beta_1=0$ ), then: 

$$E(\frac{SS_R}{df_R})=E(\frac{\sum(\hat{y}-\bar{y})^2}{1})=\sigma^2$$
And it's distributed as:
$$\frac{SS_R}{σ^2}  \sim \chi_{1}$$

---
### ANOVA

Hence, under the null:

$$F_{test}=\frac{SS_R/df_R}{SS_E/df_E}\sim F_{1,n-2}$$
--

But if the alternative is true, then:

$$E(SS_R)=\sigma^2+\beta_1^2S_{xx}$$
So typically, when null is not true, nominator will be larger than the denominator
- Hence the $F_{Stat}$ would be large

--
Intuition:
  - When model is good at explaining $y$ the explained part is larger than the unexplained part

--
- We can calculate p-value in the usual way:

$$p-value=P(F_{1,n-2} \geq F_{test})$$
---
### F-test

I will not discuss an alternative way to interpret this test, which we will use in other tests

- Let's rewrite the **F-test** in the following way:

$$F_{test}=\frac{SS_R/df_R}{SS_E/df_E}=\frac{\frac{SS_T-SS_E}{df_T-df_E}}{\frac{SS_E}{df_E}}$$
--

Think about two models trying to explain $y$
- Our model with $x_i$ $\hat{y_i}=\hat{\beta_0}+\hat{\beta_1} x_i$ (call it .blue[full model])
  - The unexplained part is measured by $SS_E=\sum(y_i-\hat{y_i})$

--
- Just intercept model $\hat{y_i}=\hat{\beta_0}=\bar{y}$ (call it .blue[restricted model])
  - The unexplained part is measured by $SS_T=\sum(y_i-\bar{y_i})$

--
- Hence $\frac{SS_T-SS_E}{df_T-df_E}$ measures by how much we decrease the unexplained part going from the reduced model to the full model
  - If it's big, it means the full model is good, and we would reject the restricted model

---

### F-test

- With one regressor, comparing model with regressor to model with just intercept is equivalent to ANOVA


--
- In this special case, $F_{test}=T_{test}^2$,  where $T_{test}$ is test for the null that the $\beta_1=0$. 

--
- With more than one regressor, we will see later, we can test whether adding predictors is helpful in explaining the variation in $y$


---

### Exercise:

From the ANOVA table of a simple linear regression model fitted with 15 observations,
we recovered the sums of squares of the residuals and the total sum of squares; namely, $SS_E$ = 52 and $SS_T$ = 152.
Using the F-test statistic, validate the significance of the regression at the 5% level. Make
The entire test approach is made explicit: hypothesis, rejection region, test statistic and its
conclusion. Use 4 decimal places.




