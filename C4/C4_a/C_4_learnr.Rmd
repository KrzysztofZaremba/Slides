---
title: "LearnR Tutorial: Estimators"
author: "Business Forecasting"
output: learnr::tutorial
runtime: shiny_prerendered
---


```{r setup, include=FALSE}
library(learnr)
library(shiny)
library(tidyverse)
library(plotly)
library(probstats4econ)

  
  # Fixed data generation
  set.seed(123) # For reproducibility
  x <- seq(-10, 10, by = 1)
  e <- rnorm(length(x), mean = 0, sd = 5)
  y_actual <- 4 + 1.65 * x + e
  
  data <- data.frame(x, y_actual)
```

---

## Example 1

We’ll work with auction data from eBay iPod sales (Rezende, 2008).  
Your task is to regress the final price on the number of bidders and find the slope. 

First, use the covariance/variance formula to find the slope and the intercept. 

\[
\hat{\beta}_1 = \frac{\text{Cov}(x,y)}{\text{Var}(x)}, 
\quad 
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
\]

```{r auction-exercise1, exercise=TRUE}
auctions$bidders

auctions$finalprice

```

Or equivalently, you can run a regression in R:

```{r auction-exercise-solution2, exercise=TRUE}
#or code for linear regression in R
x=lm(finalprice ~ bidders, data = auctions) #lm stands for linear model. the formula has form y~x and then we put data. 
summary(x)
```


```{r auction-exercise3, exercise=TRUE}
# Plug into the formula: beta0 + beta1*6

```

---

## Example 2

We’ll work with data on house sales in Ames, Iowa between 2006 and 2010.  
The dataset is limited to one-family homes with public utilities and excludes new home sales.  
We want to study the relationship between **sale price** and **lot area (in square feet)**.  

First, use the **correlation formula** to find the slope and the intercept:  

\[
\hat{\beta}_1 = r_{xy} \cdot \frac{s_y}{s_x}, 
\quad 
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
\]

where \(r_{xy}\) is the correlation between \(x\) (lot area) and \(y\) (sale price),  
and \(s_x, s_y\) are the standard deviations of \(x\) and \(y\).  

```{r housing-exercise1, exercise=TRUE}

houseprices$lotarea
houseprices$saleprice

```

Or equivalently, you can run a regression in R:

```{r housing-exercise-solution2, exercise=TRUE}
#or code for linear regression in R
x=lm(saleprice ~ lotarea, data = houseprices) #lm stands for linear model. the formula has form y~x and then we put data. 
summary(x)
```

Now, find yhat - prediction for all houses in the dataset. 


```{r housing-exercise3, exercise=TRUE}


```

Now, find e - residuals for all houses in the dataset. 


```{r housing-exercise3b, exercise=TRUE}


```
---

## Example 3

We’ll work with data on married couples in the United States (CTS Household Survey, 2003).  
Our question: **Does the body mass index (BMI) of the wife predict the BMI of the husband?**  

---

First, make a scatterplot of husband’s BMI against wife’s BMI:  

```{r bmi-exercise1, exercise=TRUE}
# Use ggplot2 to plot bmi_h (y) vs bmi_w (x)
# Hint: ggplot(married, aes(x = bmi_w, y = bmi_h)) + geom_point()

```

---

Now, run a regression of husband’s BMI on wife’s BMI:  

```{r bmi-exercise2, exercise=TRUE}


```

What explains this?

---

## Regression Decomposition

We’ll work with the Ames housing dataset.  
Regression:  

```{r housing-regression, exercise=TRUE}
# Run a regression of saleprice on lotarea
x=lm(saleprice ~ lotarea, data = houseprices)
summary(x)
```

---

Now, let’s compute the key pieces step by step.  

1. **Fitted values** (predicted sale prices):  

2. **Residuals** (errors = observed – fitted): 

```{r housing-fitted, exercise=TRUE, exercise.setup="housing-regression"}
yhat=x$fitted.values #here you will find predicted values (yhats) #this is just beta0+beta1*x

#you can do 
e=houseprices$saleprice - x$fitted.values

#or simply
e=x$residuals #here you will find residuals
```

---

3. **SST, SSR, and SSE**:  

\[
SST = \sum (y_i - \bar{y})^2, \quad
SSR = \sum (\hat{y}_i - \bar{y})^2, \quad
SSE = \sum (y_i - \hat{y}_i)^2
\]

```{r housing-sumsquares, exercise=TRUE, exercise.setup="housing-fitted"}

y=houseprices$saleprice
my=
  
SST <- sum((y - my)^2)
SSR <- sum((yhat - my)^2)
SSE <- sum((y - yhat)^2) #or sum(e^2)

c(SST = SST, SSR = SSR, SSE = SSE)
```

---

4. **Compute \(R^2\):**  

\[
R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}
\]

```{r housing-r2, exercise=TRUE, exercise.setup="housing-sumsquares"}



```


---

## Regression Decomposition 2

Now we want to learn how much of variation in prices at the auction is explained by the number of bidders. Repeat the steps from above

```{r housing-regression4, exercise=TRUE}
# Run a regression of finalprice on bidders from the auctions dataset

```

---

Now, let’s compute the key pieces step by step.  

1. **Fitted values** (predicted sale prices):  

2. **Residuals** (errors = observed – fitted): 

```{r housing-fitted4, exercise=TRUE, exercise.setup="housing-regression"}
yhat= #here you will find predicted values (yhats)



e= #here you will find residuals
```

---

3. **SST, SSR, and SSE**:  

\[
SST = \sum (y_i - \bar{y})^2, \quad
SSR = \sum (\hat{y}_i - \bar{y})^2, \quad
SSE = \sum (y_i - \hat{y}_i)^2
\]

```{r housing-sumsquares4, exercise=TRUE, exercise.setup="housing-fitted"}

y=
my=
  
SST <-
SSR <- 
SSE <- 

c(SST = SST, SSR = SSR, SSE = SSE)
```

---

4. **Compute \(R^2\):**  

\[
R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}
\]

```{r housing-r24, exercise=TRUE, exercise.setup="housing-sumsquares4"}



```

---

## Scaling of Variables

We’ll work with data on married couples in the U.S. (CTS Household Survey, 2003).  
Our outcome is **family income**, and our predictor is **husband’s age**.  

---

#### Step 1: Regression with original units  
Family income in **dollars**, husband’s age in **years**.  

\[
famincome_i = \beta_0 + \beta_1 \cdot age\_h_i + u_i
\]

```{r married-exercise1, exercise=TRUE}
# Run a regression of family income (dollars) on husband's age (years)
# lm(famincome ~ age_h, data = married)
```

Visualize:  

```{r married-plot1, exercise=TRUE}
# ggplot(married, aes(x = age_h, y = famincome)) +
#   geom_point(alpha=0.2) +
#   geom_smooth(method="lm", se=FALSE, color="blue") +
#   labs(x="Husband's age (years)", y="Family income (dollars)")
```

---

#### Step 2: Rescale income to **thousands of dollars**  

Transformation:  

\[
famincome\_thousands = \frac{famincome}{1000}
\]

New regression:  

\[
famincome\_thousands = \beta_0^\* + \beta_1^\* \cdot age\_h + u
\]



```{r m2, exercise=TRUE}
# married$famincome_thousands = married$famincome / 1000

# lm(famincome_thousands ~ age_h, data = married)

# ggplot(married, aes(x = age_h, y = famincome_thousands)) +
#   geom_point(alpha=0.2) +
#   geom_smooth(method="lm", se=FALSE, color="blue") +
#   labs(x="Husband's age (years)", y="Family income (thousands of $)")
```

---

#### Step 3: Rescale age to **months**  

Transformation:  

\[
age\_h\_months = age\_h \times 12
\]

New regression:  

\[
famincome = \gamma_0 + \gamma_1 \cdot age\_h\_months + u
\]

How does the correlation formula changes?

```{r married-exercise4, exercise=TRUE}
# married$age_h_months = married$age_h * 12


# lm(famincome_thousands ~ age_h_months, data = married)


# ggplot(married, aes(x = age_h_months, y = famincome_thousands)) +
#   geom_point(alpha=0.2) +
#   geom_smooth(method="lm", se=FALSE, color="blue") +
#   labs(x="Husband's age (months)", y="Family income (thousands of $)")
```


---

### Reflection  

- Did R squared change?
