---
title: 'Class 3b: Review of concepts in Probability and Statistics'
author: "Business Forecasting"
output:
  xaringan::moon_reader:
    self_contained: true
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: true
      
---   
<style type="text/css">
.remark-slide-content {
    font-size: 20px;
}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,dpi=300)
library(shiny)
library(ggplot2)
library(forecast)
library(plotly)
library(dplyr)
library(igraph)
library(reshape)
library(spData)
library(leaflet)
library(readr)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(hrbrthemes)
library(viridis)
library(gapminder)
library(knitr)
library(kableExtra)
library(DT)

```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(base_color = "#43418A", 
colors = c(
  red = "#f34213",
  purple = "#3e2f5b",
  orange = "#ff8811",
  green = "#136f63",
  blue = "#1E90FF",
  white = "#FFFFFF"
))
load("sample_listing.Rda")
```


---
layout: false
class: inverse, middle

# Hypothesis Testing

---
### Hypothesis Testing

Let's go back to our question. Can we get a higher price if cleanliness level 4.5?


This type of questions can be answered with hypothesis testing where we try to see which one of competing hypothesis is more likely to be true. 

- .blue[Hypothesis] is a claim about a parameter or a distribution. 

--
1. $H_0$ **Null Hypothesis**: 
  - Claim to be tested, skeptical perspective

--
      - Prices of clean and dirty apartments are equal 
      - $H_0: \mu_c=\mu_d$

--
2. $H_A$ **Alternative Hypothesis**:
  - An alternative idea under consideration

--
      - Prices of clean apartments are higher than prices of dirty apartments
       - $H_A: \mu_c>\mu_d$

--

- Assume that the null is true and see if there is enough of data to reject the null in favor of the alternative.
- You either reject the null, or fail to reject it (but not accept it)
---
### Hypothesis Testing

Some examples of hypotheses testing:

- You released a new promotion to some customers (10% off) and you want to test if people who got the promotion spend different amount of money than those who didn't

--
  - $H_0$: People who got the promotion spend the same as people who didn't get the promotion
      - $H_0: \mu_p =  \mu_n$
  - $H_A$: People who got the promotion spend more than people who didn't get the promotion
      - $H_A: \mu_p \neq \mu_n$

--

- You designed a new healthy snack and want to test whether it needs "excessive sugar" sticker. It gets the sticker if it exceeds 100 this much of sugar. You take a sample and test

--
  - $H_0$: The average sugar content is 100 or less
      - $H_0: \mu_s =  100$
  - $H_A$: The average sugar content is more than 100
      - $H_A: \mu_s \geq  100$

---
### Hypothesis Testing

- Suppose we calculated the sample mean sugar content. It's either below or above 100

--

- Why would we need a fancy test

--

- Because we only have a sample, but we want to learn about the population parameter!

--

- Hypothesis are always about parameters! Never about sample sample statistics

---
### Testing procedure


**Test procedure** is a rule based on a sample data whether to reject the null or not.
 
It usually relies on two elements:
 - .blue[Test statistics] 
  - Function of the sample data summarizing evidence for or against a hypothesis 
  - On which we base the decision
 - .blue[Rejection region] 
  - Values of the test statistics such that we would reject the null if we observe them
  - Typically threshold based:
  - Such as reject $H_0$ if the test statistic > k, where k is some threshold
 

---
### Testing procedure: Example

Let's go back to the sugar example. Suppose we have a sample $n=36$.

 - .blue[Test statistics] 
  - Our test statistic would be:
  $$z=\frac{\bar{x}-100}{\underbrace{s_x/\sqrt 36}_{SE}}$$

 - .blue[Rejection region] 
  - For a 5% significance level, our rejection region would be: 
  - $\{1.645, \infty \}$
  
.center[
```{r}
magick::image_read_pdf("Rejection.pdf",
                       pages = 1)
```
]


---
### Testing procedure: Example


- Suppose $s_x=20$. What's the lowest $\bar{x}$ for which we would reject?

--

- $1.645=\frac{\bar{x}_{min}-100}{20/\sqrt 36} \rightarrow \bar{x}_{min}=105.483$

--

- Why don't we reject even if $\bar{x}=103$

--

- Because there could be randomness in the sample

- So even if the $\mu_s=100$ we could get a sample with $\bar{x}>100$



---

### Decision errors

- Suppose that the null is true and $\mu_s=100$

--
- You take a sample of 36. 

--
- But you were very unlucky in your sampling, so by chance you actually got a sample with those rare packages with  a lot of sugar and your $\bar{x}=106$

--
- According to the test procedure, you reject the null

--
- You make an error

--
- What's the probability of making such error?

$$\alpha=P(\text{Type 1 error})=\underbrace{P_{H_0}(\bar{X}>105.483)}_{\text{Pr. if null is true}}=P(Z>1.645)=1-\underbrace{\Phi(1.645)}_{\text{Normal CDF}}$$


```{r, warning=FALSE, fig.height=2.5, out.width='100%'}

curve(dnorm(x, 100, 20/6),
      xlim = c(90, 110),
      yaxs = "i",
      xlab = "Mean sugar content in the sample if null is true",
      ylab = "",
      lwd = 2,
      axes = "F")

# add x-axis
axis(1, 
     at = c(95,100, 105.483), 
     padj = 0.75,
     labels = c(95,expression(100),
                expression(105.483)))


polygon(x = c(105.483, seq(105.483, 110, 0.01), 110),
        y = c(0, dnorm(seq(105.483, 110, 0.01), 100, 20/6), 0),
        col = "steelblue")

# add vertical line at the mean (mu)
abline(v = 105.483, col = "red", lwd = 2)


text(108.1, 0.075, "Rejection \n region", col = "black", cex = 1.5)
text(106, 0.015, expression(alpha), col = "white", cex = 1.5)

```

---

### Decision Errors


|             | Do not reject $H_0$ | Reject $H_0$ in favor of $H_A$ |
| ----------- | ------------------- | ------------------------------- |
| $H_0$ true  | Okay                | Type 1 Error                    |
| $H_A$ true  | Type 2 Error        | Okay                            |

- $H_0$: You are not pregnant
- $H_1$: You are pregnant

--
.center[
<img src="pregnancy.jpg" style="width:50%">
]
---
### Decision Errors

- .blue[Type 1 error]: reject null when $H_0$ is true
  - Conclude that people spend more when they got free shipping, while in reality they spend the same as those who didn't

--
- .blue[Type 2 error]: don't reject null when $H_A$ is true
  - Conclude that people who got free shipping spend the same as those without free shipping, while those with free shipping spend more

--

- Imagine a trial of a murderer and suppose $H_0$ is innocent, and $H_1$ is guilty. 
- Describe error type 1 and 2 in these scenarios

--
  - Type 1: Convict an innocent
  - Type 2: Do not convict a murderer


--
- Probability of making type 1 error is $\alpha$
  - It's also called .blue[size], or .blue[significance level] of the test
  - We don't want to incorrectly reject null more than $\alpha$*100 percent of times. 

--
- Probability of making type 2 error is $\beta$
  - To calculate $\beta$, you need to know the true distribution
---
### Trade-off between type 1 and 2 errors
- When you decrease probability of error type 1, you increase the probability of error type 2

`r knitr::include_url('https://shiny.rit.albany.edu/stat/betaprob/', height='450px')`
Source: https://shiny.rit.albany.edu/stat/betaprob/

---
### Power of a test

- Power of a test is the probability of rejecting if alternative is true
- Power will be different for any possible value of the alternative
- Notice that 

$$Power=P_{H_A}(Reject)=1-\underbrace{P_{H_A}(\text{Not Reject})}_{\text{type 2 error}}=1-\beta$$

Source: https://shiny.rit.albany.edu/stat/betaprob/

---
### Power of a test 
- Let's go back to sugar example. Our rule was reject $H_0$ if $z>1.645$. 
- Suppose $\mu_s=110$ and $\sigma=20$. Let's calculate power of the test:

$$\scriptsize P_{H_A}(Z_{test}>1.645)=P_{H_A}(\frac{\bar{X}-100}{\underbrace{ \sigma_x/ \sqrt 36}_{SE}}>1.645)=P_{H_A}(\frac{\bar{X}-\mu_0}{\underbrace{ \sigma_x/ \sqrt n}_{SE}}>z_{\alpha})=P_{H_A}(\bar{X}>\mu_0+z_{\alpha}\frac{\sigma_x}{\sqrt n})=P_{H_A}(\bar{X}>105.483)$$
--

- To calculate this probability, let's standarize $\bar{X}$:
  - We subtract the true mean of the distribution and the standard deviation

$$\scriptsize P_{H_A}(\frac{\bar{X}-\mu_s}{ \sigma_x/ \sqrt n}>\frac{\mu_0-\mu_s}{\sigma_x/ \sqrt n}+z_{\alpha})=P_{H_A}(\frac{\bar{X}-\mu_s}{ \sigma_x/ \sqrt n}>\frac{-10}{20/ \sqrt 36}+1.645)=P_{H_A}(\underbrace{\frac{\bar{X}-\mu_s}{ \sigma_x/ \sqrt n}}_{st.normal}>-1.355)=0.9115$$
Then: $\scriptsize \beta=1-0.9115=0.0885$
 
--

Power increases if:
- n increases
- Alternative is further away from the null
- $\sigma$ decreases
- $\alpha$ increases

---
### Link between Tests and Confidence Intervals

Imagine a test for a parameter $\mu$ with hypothesis $H_0=4$ and $H_A \neq 4$. 

- Suppose you draw a sample of 49 units with a mean $\bar{x}=3$ and standard deviation is $s=3$
- Our test statistic is $z_{test}=\frac{3-4}{3/7}=-2.34$
- Our critical values at $\alpha=0.05$ are -1.96 and 1.96, hence we reject

--
- We can also calculate the 95% confidence interval for our sample mean:

$$\small \{3-1.96\frac{1}{\sqrt 49}, 3+1.96\frac{1}{\sqrt 49}\}=\{2.16, 3.84\}$$
- It does not contain the null hypothesis


---
### Link between Tests and Confidence Intervals

- More generally: the $1-\alpha$ confidence interval doesn't contain null = null would be rejected with a test of significance $\alpha$

-Mathematically: 

  - Let $\mu_0$ be the null hypothesis. We reject (in two sided test) the null at $\alpha$ if 
  $$\scriptsize \left| \small \frac{\bar{X}-\mu_0}{ s/ \sqrt n} \right|>z_{\frac{\alpha}{2}}$$
  
  - Which can be rewritten as: 

  $$\scriptsize  \mu_0<\bar{X}-z_{\frac{\alpha}{2}}\frac{s}{\sqrt n}  \qquad or  \qquad \mu_0>\bar{X}+z_{\frac{\alpha}{2}}\frac{s}{\sqrt n}$$

  
---

Q if it's rejected at two sided test, then it would be also rejected at one sided test 


  
---

### General procedure:

1. Determine the the null and the alternative hypothesis

--
2. Collect your sample of independent observations

--
3. Choose the appropriate test

--
4. Calculate test statistic

--
5. Compare it to the rejection region

--
5. Reject or not your null hypothesis



--
Let's talk about **appropriate tests**!
---

### Appropriate tests

Choice of the test statistic and rejection region will depend on:

- **Type of data we are testing?**
  - .blue[Single sample]
      - Example of hypothesis: $H_0: \mu=\mu_0$ vs $H_A: \mu \neq \mu_0$ 
  - .blue[Two independent samples]
      - Example of hypothesis: $H_0: \mu_{s1}=\mu_{s2}$ vs $H_A: \mu_{s1} \neq \mu_{s2}$
  - .blue[Paired data]
      - Example of hypothesis: $H_0: \mu_{d1}=\mu_{d2}$ vs $H_A: \mu_{d1} \neq \mu_{d2}$
  

- **Can we estimate standard deviation well?**
  - .blue[Yes]
      - Large sample
      - Known standard deviation
  - .blue[No]
      - Small sample but normal distribution
  

---

layout: false
class: inverse, middle

# Hypothesis Testing: Single Sample

---

### Single Sample Test for Mean

**General idea:** 
- We test if the parameter is equal/larger/smaller than some concrete value
  - Ex: $H_0: \mu=3$ vs $H_A: \mu \neq 3$

--
- .blue[Test statistic]: normalized sample mean

$$\text{test statistic}=\frac{\bar{X}-\mu_0}{SE}$$
 
  - Where SE is standard error and depends on estimate of standard deviation
      - If standard deviation known: $\small SE=\frac{\sigma}{\sqrt n}$
      - If standard deviation not known: $\small SE=\frac{s}{\sqrt n}$

--
- .blue[Rejection region]: depends on the distribution 
  - If large sample or known variance $\small \text{test statistic}=Z_{test} \sim N(0,1)$
      - Critical values come from standard normal distribution
  - If small sample with knormal distribution $\small \text{test statistic}=T_{test} \sim t(n-1)$
      - Critical values come from student t with n-1 degrees of freedom





-
---
### Single Sample - Mean - Two Sided Test

**Hypothesis:**
$H_0: \mu=\mu_0$  and 
$H_A: \mu \neq \mu_0$

**Rejection Region:**
For a test of significance level $\alpha$, reject

- If large sample or known variance
$$\small \frac{\bar{X}-\mu_0}{SE}<-z_\frac{\alpha}{2} \qquad or  \qquad \frac{\bar{X}-\mu_0}{SE}>z_\frac{\alpha}{2}$$
--

- If small sample from normal
$$\small \frac{\bar{X}-\mu_0}{SE}<-t_{(n-1),\frac{\alpha}{2}} \qquad or  \qquad \frac{\bar{X}-\mu_0}{SE}>t_{(n-1),\frac{\alpha}{2}}$$
--
Where 
- $z_\frac{\alpha}{2}$ and $t_{(n-1),\frac{\alpha}{2}}$ are  $\small (1-\frac{\alpha}{2})$ quantiles of standard normal and t-students with n-1 degrees of freedom

---
### Single Sample - Mean - Two Sided Test

.center[
```{r, warning=FALSE, fig.height=4, out.width='100%'}
# plot the standard normal density on the interval [-4,4]
curve(dnorm(x),
      xlim = c(-3, 3),
      main = "Distribution of the statistic under the null (standard normal case)",
      yaxs = "i",
      xlab = "Test Statistic",
      ylab = "",
      lwd = 2,
      axes = "F")


# add x-axis
axis(1, 
     at = c(-1.96,  0,  1.96), 
     padj = 0.75,
     labels = c(expression(z[frac(alpha,2)]),
                expression(0),
                expression(-z[frac(alpha,2)])))

# add a vertical line at the mean (mu)

# shade the tails for the 2.5% regions
polygon(x = c(-3, seq(-3, -1.96, 0.01), -1.96),
        y = c(0, dnorm(seq(-3, -1.96, 0.01)), 0),
        col = "steelblue", alpha = 0.2)

polygon(x = c(1.96, seq(1.96, 3, 0.01), 3),
        y = c(0, dnorm(seq(1.96, 3, 0.01)), 0),
        col = "steelblue", alpha = 0.2)

# add vertical line at the mean (mu)
abline(v = 0, col = "red", lwd = 2)

# add the "2.5%" labels on tails
text(-2.2, 0.1, expression(alpha/2), col = "black", cex = 1.5)
text(2.2, 0.1, expression(alpha/2), col = "black", cex = 1.5)
```
]

---

### Single Sample - Mean - One sided - Case 1

**Hypothesis:**
$H_0: \mu=\mu_0$ and 
$H_A: \mu < \mu_0$

- This includes any null hypothesis like $\mu \geq \mu_0$. Why?

--
- If you rejected $\mu_0$ at $\alpha$, you would reject for sure anything larger than $\mu_0$


--
**Rejection Region:**
For a test of significance level $\alpha$, reject

- If large sample or known variance
$$\small \frac{\bar{X}-\mu_0}{SE}<-z_\alpha$$
--

- If small sample from normal
$$\small \frac{\bar{X}-\mu_0}{SE}<-t_{(n-1),\alpha}$$
--
Where 
- $z_\alpha$ and $t_{(n-1),\alpha}$ are  $\small (1-\alpha)$ quantiles of standard normal and t-students with n-1 degrees of freedom

---
### Single Sample - Mean - One sided - Case 1

.center[
```{r, warning=FALSE, fig.height=3, out.width='100%'}
# plot the standard normal density on the interval [-4,4]
curve(dnorm(x),
      xlim = c(-3, 3),
      main = "Distribution of the statistic under the null (standard normal case)",
      yaxs = "i",
      xlab = "Test Statistic",
      ylab = "",
      lwd = 2,
      axes = "F")


# add x-axis
axis(1, 
     at = c(-1.645,  0 ,1.645), 
     padj = 0.75,
     labels = c(expression(-z[alpha]),
                expression(0),
                expression(z[alpha])))

# add a vertical line at the mean (mu)

# shade the tails for the 2.5% regions
polygon(x = c(-3, seq(-3, -1.645, 0.01), -1.645),
        y = c(0, dnorm(seq(-3, -1.645, 0.01)), 0),
        col = "steelblue", alpha = 0.2)


# add vertical line at the mean (mu)
abline(v = 0, col = "red", lwd = 2)

# add the "2.5%" labels on tails
text(-2.2, 0.1, expression(alpha), col = "black", cex = 1.5)

```
]

- .blue[Intuition]: 
  -  We never reject $H_0$ if $\bar{X} \geq \mu_0$.
  -  We reject $H_0$ if $\bar{X}$ is sufficiently smaller than $\mu_0$
  -  If null is true ( $\mu=\mu_0$ ) then we reject with probability
$$\small P(\frac{\bar{X}-\mu_0}{s/\sqrt n}<-z_\alpha)=P(Z<-z_\alpha)=\alpha$$


---

### Single Sample - Mean - One sided - Case 2

**Hypothesis:**
$H_0: \mu=\mu_0$ and 
$H_A: \mu > \mu_0$

- This includes any null hypothesis like $\mu \leq \mu_0$. Why?

--
- If you rejected $\mu_0$ at $\alpha$, you would reject for sure anything smaller than $\mu_0$


--
**Rejection Region:**
For a test of significance level $\alpha$, reject

- If large sample or known variance
$$\small \frac{\bar{X}-\mu_0}{SE}>z_\alpha$$
--

- If small sample from normal
$$\small \frac{\bar{X}-\mu_0}{SE}>t_{(n-1),\alpha}$$
--
Where 
- $z_\alpha$ and $t_{(n-1),\alpha}$ are  $\small (1-\alpha)$ quantiles of standard normal and t-students with n-1 degrees of freedom

---
### Single Sample - Mean - One sided - Case 2

.center[
```{r, warning=FALSE, fig.height=3, out.width='100%'}
# plot the standard normal density on the interval [-4,4]
curve(dnorm(x),
      xlim = c(-3, 3),
      main = "Distribution of the statistic under the null (standard normal case)",
      yaxs = "i",
      xlab = "Test Statistic",
      ylab = "",
      lwd = 2,
      axes = "F")


# add x-axis
axis(1, 
     at = c(-1.645,  0 ,1.645), 
     padj = 0.75,
     labels = c(expression(-z[alpha]),
                expression(0),
                expression(z[alpha])))

# add a vertical line at the mean (mu)

# shade the tails for the 2.5% regions
polygon(x = c(1.645, seq(1.645, 3, 0.01), 3),
        y = c(0, dnorm(seq(1.645, 3, 0.01)), 0),
        col = "steelblue", alpha = 0.2)


# add vertical line at the mean (mu)
abline(v = 0, col = "red", lwd = 2)

# add the "2.5%" labels on tails
text(2.2, 0.1, expression(alpha), col = "black", cex = 1.5)

```
]

- .blue[Intuition]: 
  -  We never reject $H_0$ if $\bar{X} \leq \mu_0$.
  -  We reject $H_0$ if $\bar{X}$ is sufficiently larger than $\mu_0$
  -  If null is true ( $\mu=\mu_0$ ) then we reject with probability
$$\small P(\frac{\bar{X}-\mu_0}{s/\sqrt n}<-z_\alpha)=P(Z<-z_\alpha)=\alpha$$


---
### Tests for Variance
Test for variance has the same idea, but different distribution of the test statistic.
Suppose you have an IID sample from .blue[normal distribution] 

**Test statistic** for $H_0: \sigma=\sigma_0$  and its distribution if null is true is:
$$\small \chi_{test}=  \frac{(n-1)S^2}{\sigma _{0}^{2}}\sim \chi_{n-1}$$
It is distributed as a Chi-square with n-1 degrees of freedom if null is true:

--
**Rejection regions:**
1. $\small H_A: \sigma \neq\sigma_0$
  - Reject null if  $\small \chi_{test}<\chi_{\frac{\alpha}{2},n-1}$ or $\small \chi_{test}>\chi_{1-\frac{\alpha}{2},n-1}$

--
2. $\small H_A: \sigma \leq \sigma_0$
  - Reject null if $\small \chi_{test}<\chi_{\alpha,n-1}$

--
3. $\small H_A: \sigma \geq \sigma_0$
  - Reject null if $\small \chi_{test}>\chi_{1-\alpha,n-1}$

--
Where:
- $\small \chi_{\alpha,n-1}$ is $\small \alpha$ quantile of $\chi_{n-1}$
- $\small \chi_{1-\alpha,n-1}$ is $\small 1-\alpha$ quantile of $\chi_{n-1}$
- $\small \chi_{df}$ distribution is not symmetric around 0!


---

**Exercise**

Suppose you are a ham producer. As you need to stick to the nutritional guidelines, the standard deviation of the fat content in your product needs to be less than 0.1. You take a sample of 16 hams and you want to test this. 

--
1. Write down the hypotheses of the test

--
- Remember test statistic is about variance
- Pay attention to which direction you test! You can't confirm the null with a test. 

--
2. What is type 1 and type 2 error in this context


--
3. Test the claim at 5% significance level. State assumptions you need to implement it


--
4. What is 95% confidence interval for variance
---

### P-values

**Technically:**
The P-value is the probability (so between 0 and 1), under the null hypothesis, of obtaining a value of the test statistic at least as contradictory to H0 as the value calculated from the available sample. 

Depends on:
- Your hypothesis
- Calculated test statistic

- Suppose the test statistic under $H_0$ is N(0,1)
- One sided tests:
  - For  $H_A: \mu>\mu_0$ $p-value=P(Z_{Test}>(computed-test-statistic))$
  - For  $H_A: \mu<\mu_0$ $p-value=P(Z_{Test}<(computed-test-statistic))$
- Two sided test:
  - For  $H_A: \mu \neq\mu_0$ $p-value=2P(Z_{Test}>|(computed-test-statistic)|)$

--
- If the test statistic under $H_0$ is distributed student's t, find appropriate probability in the student's t table. 

---
**Example**: 
- We test for $H_0: \mu=5$ and $H_A: \mu>5$ 
- Suppose in sample of 36 we have $\x=5.5$ and $s=2$. 
- Test statistic in this case is distributed normally
$$p-value=P(Z>\frac{5.5-5}{2/6})=P(Z>1.5)=0.066$$


--

**Intuitively:**
The probability that a statistic we calculated (or more extreme value) could arise just by chance if null is true

---
### P-value in two-sided test

`r knitr::include_url('https://rpsychologist.com/pvalue/', height='400px')`
Source: https://rpsychologist.com/pvalue/

---
### P-Values and test significance

- In our example we calculated the p-value of 0.06.
- Would the test at $\alpha=0.05$ reject the null?

--
- Any test with the significance level $\alpha>p-value$ would reject the null


**Another way to think about p-values**
- The P-value is the smallest significance level $\alpha$ at which the $H_0$
can be rejected


---
layout: false
class: inverse, middle

# Hypothesis Testing: Two samples

---

### Two Samples for the Difference in Means

**General idea:** 
- We test the difference between .blue[population means] in two populations
  - Ex: $H_0: \mu_1-\mu_2=100$ vs $H_A: \mu_1-\mu_2 \neq 100 $
  - Ex: $H_0: \mu_1=\mu_2 \rightarrow \mu_1-\mu_2=0$ vs $H_0: \mu_1>\mu_2 \rightarrow \mu_1-\mu_2>0$
--
  - Test for no difference:
      - Ex: $H_0: \mu_1=\mu_2 \rightarrow \mu_1-\mu_2=0$ vs $H_0: \mu_1 \neq \mu_2 \rightarrow \mu_1-\mu_2 \neq 0$

--
- .blue[Test statistic]: normalized sample mean

$$\text{test statistic}=\frac{\bar{X}-\mu_0}{SE}$$
 
  - Where SE is standard error and depends on estimate of standard deviation
      - If standard deviation known: $\small SE=\frac{\sigma}{\sqrt n}$
      - If standard deviation not known: $\small SE=\frac{s}{\sqrt n}$

--
- .blue[Rejection region]: depends on the distribution 
  - If large sample or known variance $\small \text{test statistic}=Z_{test} \sim N(0,1)$
      - Critical values come from standard normal distribution
  - If small sample with knormal distribution $\small \text{test statistic}=T_{test} \sim t(n-1)$
      - Critical values come from student t with n-1 degrees of freedom





-
---
### Single Sample - Mean - Two Sided Test

**Hypothesis:**
$H_0: \mu=\mu_0$  and 
$H_A: \mu \neq \mu_0$

**Rejection Region:**
For a test of significance level $\alpha$, reject

- If large sample or known variance
$$\small \frac{\bar{X}-\mu_0}{SE}<-z_\frac{\alpha}{2} \qquad or  \qquad \frac{\bar{X}-\mu_0}{SE}>z_\frac{\alpha}{2}$$
--

- If small sample from normal
$$\small \frac{\bar{X}-\mu_0}{SE}<-t_{(n-1),\frac{\alpha}{2}} \qquad or  \qquad \frac{\bar{X}-\mu_0}{SE}>t_{(n-1),\frac{\alpha}{2}}$$
--
Where 
- $z_\frac{\alpha}{2}$ and $t_{(n-1),\frac{\alpha}{2}}$ are  $\small (1-\frac{\alpha}{2})$ quantiles of standard normal and t-students with n-1 degrees of freedom

---
### Single Sample - Mean - Two Sided Test

.center[
```{r, warning=FALSE, fig.height=4, out.width='100%'}
# plot the standard normal density on the interval [-4,4]
curve(dnorm(x),
      xlim = c(-3, 3),
      main = "Distribution of the statistic under the null (standard normal case)",
      yaxs = "i",
      xlab = "Test Statistic",
      ylab = "",
      lwd = 2,
      axes = "F")


# add x-axis
axis(1, 
     at = c(-1.96,  0,  1.96), 
     padj = 0.75,
     labels = c(expression(z[frac(alpha,2)]),
                expression(0),
                expression(-z[frac(alpha,2)])))

# add a vertical line at the mean (mu)

# shade the tails for the 2.5% regions
polygon(x = c(-3, seq(-3, -1.96, 0.01), -1.96),
        y = c(0, dnorm(seq(-3, -1.96, 0.01)), 0),
        col = "steelblue", alpha = 0.2)

polygon(x = c(1.96, seq(1.96, 3, 0.01), 3),
        y = c(0, dnorm(seq(1.96, 3, 0.01)), 0),
        col = "steelblue", alpha = 0.2)

# add vertical line at the mean (mu)
abline(v = 0, col = "red", lwd = 2)

# add the "2.5%" labels on tails
text(-2.2, 0.1, expression(alpha/2), col = "black", cex = 1.5)
text(2.2, 0.1, expression(alpha/2), col = "black", cex = 1.5)
```
]

---

### Single Sample - Mean - One sided - Case 1

**Hypothesis:**
$H_0: \mu=\mu_0$ and 
$H_A: \mu < \mu_0$

- This includes any null hypothesis like $\mu \geq \mu_0$. Why?

--
- If you rejected $\mu_0$ at $\alpha$, you would reject for sure anything larger than $\mu_0$


--
**Rejection Region:**
For a test of significance level $\alpha$, reject

- If large sample or known variance
$$\small \frac{\bar{X}-\mu_0}{SE}<-z_\alpha$$
--

- If small sample from normal
$$\small \frac{\bar{X}-\mu_0}{SE}<-t_{(n-1),\alpha}$$
--
Where 
- $z_\alpha$ and $t_{(n-1),\alpha}$ are  $\small (1-\alpha)$ quantiles of standard normal and t-students with n-1 degrees of freedom

---
### Single Sample - Mean - One sided - Case 1

.center[
```{r, warning=FALSE, fig.height=3, out.width='100%'}
# plot the standard normal density on the interval [-4,4]
curve(dnorm(x),
      xlim = c(-3, 3),
      main = "Distribution of the statistic under the null (standard normal case)",
      yaxs = "i",
      xlab = "Test Statistic",
      ylab = "",
      lwd = 2,
      axes = "F")


# add x-axis
axis(1, 
     at = c(-1.645,  0 ,1.645), 
     padj = 0.75,
     labels = c(expression(-z[alpha]),
                expression(0),
                expression(z[alpha])))

# add a vertical line at the mean (mu)

# shade the tails for the 2.5% regions
polygon(x = c(-3, seq(-3, -1.645, 0.01), -1.645),
        y = c(0, dnorm(seq(-3, -1.645, 0.01)), 0),
        col = "steelblue", alpha = 0.2)


# add vertical line at the mean (mu)
abline(v = 0, col = "red", lwd = 2)

# add the "2.5%" labels on tails
text(-2.2, 0.1, expression(alpha), col = "black", cex = 1.5)

```
]

- .blue[Intuition]: 
  -  We never reject $H_0$ if $\bar{X} \geq \mu_0$.
  -  We reject $H_0$ if $\bar{X}$ is sufficiently smaller than $\mu_0$
  -  If null is true ( $\mu=\mu_0$ ) then we reject with probability
$$\small P(\frac{\bar{X}-\mu_0}{s/\sqrt n}<-z_\alpha)=P(Z<-z_\alpha)=\alpha$$


---

### Single Sample - Mean - One sided - Case 2

**Hypothesis:**
$H_0: \mu=\mu_0$ and 
$H_A: \mu > \mu_0$

- This includes any null hypothesis like $\mu \leq \mu_0$. Why?

--
- If you rejected $\mu_0$ at $\alpha$, you would reject for sure anything smaller than $\mu_0$


--
**Rejection Region:**
For a test of significance level $\alpha$, reject

- If large sample or known variance
$$\small \frac{\bar{X}-\mu_0}{SE}>z_\alpha$$
--

- If small sample from normal
$$\small \frac{\bar{X}-\mu_0}{SE}>t_{(n-1),\alpha}$$
--
Where 
- $z_\alpha$ and $t_{(n-1),\alpha}$ are  $\small (1-\alpha)$ quantiles of standard normal and t-students with n-1 degrees of freedom

---
### Single Sample - Mean - One sided - Case 2

.center[
```{r, warning=FALSE, fig.height=3, out.width='100%'}
# plot the standard normal density on the interval [-4,4]
curve(dnorm(x),
      xlim = c(-3, 3),
      main = "Distribution of the statistic under the null (standard normal case)",
      yaxs = "i",
      xlab = "Test Statistic",
      ylab = "",
      lwd = 2,
      axes = "F")


# add x-axis
axis(1, 
     at = c(-1.645,  0 ,1.645), 
     padj = 0.75,
     labels = c(expression(-z[alpha]),
                expression(0),
                expression(z[alpha])))

# add a vertical line at the mean (mu)

# shade the tails for the 2.5% regions
polygon(x = c(1.645, seq(1.645, 3, 0.01), 3),
        y = c(0, dnorm(seq(1.645, 3, 0.01)), 0),
        col = "steelblue", alpha = 0.2)


# add vertical line at the mean (mu)
abline(v = 0, col = "red", lwd = 2)

# add the "2.5%" labels on tails
text(2.2, 0.1, expression(alpha), col = "black", cex = 1.5)

```
]

- .blue[Intuition]: 
  -  We never reject $H_0$ if $\bar{X} \leq \mu_0$.
  -  We reject $H_0$ if $\bar{X}$ is sufficiently larger than $\mu_0$
  -  If null is true ( $\mu=\mu_0$ ) then we reject with probability
$$\small P(\frac{\bar{X}-\mu_0}{s/\sqrt n}<-z_\alpha)=P(Z<-z_\alpha)=\alpha$$


---

Problem: our initial question! Is price of clean apartments higher than the price of dirty apartments?



---



ASLO TALK ABOUT mu1-mu2=m not only 0


Two sided test:

$H_0: \mu=\mu_0$
$H_A: \mu \neq \mu_0$

For test of size/significance level $\alpha$, reject if 

$$-z_{\alpha/2}>\frac{\bar{X}-\mu_0}{s/\sqrt n} \qquad\text{or}\qquad \frac{\bar{X}-\mu_0}{s/\sqrt n}>z_{\alpha/2}$$ 

Where $z_{\alpha/2}$ is such that $P(Z>z_{\alpha/2})=\alpha/2$.
So for 95% confidence, we have $P(Z>z_{0.025})=P(Z>1.96)=0.025$:
hence our rejection region would be below -1.96 and above 1.96
 

Comments: 
Under the null, probability of making the error is still $\alpha$
But now we have two rejection regions, so $\alpha$ is distributed equally among them

Consider a 95% test and our test statistic is 1.8
Would we reject it in two sided test?
What about one sided test? What direction of hypotheses?


```{r, warning=FALSE, fig.height=2.5, out.width='100%'}


curve(dnorm(x),
      xlim = c(-4, 4),
      main = "Standard normal",
      yaxs = "i",
      xlab = "z",
      ylab = "",
      lwd = 2,
      axes = "F")

# add x-axis
axis(1, 
     at = c(-1.96, 0, 1.96), 
     padj = 0.75,
     labels = c(expression(-z[alpha/2]),
                expression(0),
                expression(z[alpha/2])))

# shade the tails for the 2.5% regions
polygon(x = c(-4, seq(-4, -1.96, 0.01), -1.96),
        y = c(0, dnorm(seq(-4, -1.96, 0.01)), 0),
        col = "steelblue", alpha = 0.2)

polygon(x = c(1.96, seq(1.96, 4, 0.01), 4),
        y = c(0, dnorm(seq(1.96, 4, 0.01)), 0),
        col = "steelblue", alpha = 0.2)

# add vertical line at the mean (mu)
abline(v = 0, col = "red", lwd = 2)

# add the "2.5%" labels on tails
text(-3, 0.05, expression(alpha/2), col = "steelblue", cex = 1.5)
text(3,  0.05, expression(alpha/2), col = "steelblue", cex = 1.5)

text(-3.1, 0.2, "Rejection \n region", col = "black", cex = 1.5)
text(3.2, 0.2, "Rejection \n region", col = "black", cex = 1.5)


```
---




test statistic is typically standardized value of the sample mean Z

-> go one by one with 3 types of hypotheses and corresponding images

- show interactively as well, what happens to the tests when I manipulate various parts and summarize it. 

- normal population with known sigma

-> just show what's the test statistic and rejection regions for each type of hypothesis

- small sample with normal population with unknown sigma

-> just show what's the test statistic and rejection region

---

exercise: 
1. [32 puntos] El gerente de una agencia de venta de autom´oviles est´a evaluando un nuevo plan de
bonificaciones con el objetivo de incrementar el volumen de ventas mensual por vendedor.
Actualmente, se tiene conocimiento de que para un mes dado, el volumen medio es de 14 autom´oviles
por vendedor. El gerente desea realizar un estudio para ver si el nuevo plan de bonificaciones realmente
incrementa el volumen de ventas. Para lo anterior, selecciona una muestra aleatoria de 41 vendedores
quienes vender´an durante un mes bajo el nuevo plan de bonificaciones.
a) [4 puntos] Establezca la hip´otesis nula m´as adecuada si se quisiera hacer una prueba de hip´otesis
para este estudio.
b) [6 puntos] Si se realizara la prueba de hip´otesis, comente la conclusi´on resultante, en t´erminos de
la decisi´on de negocios, que se tendr´ıa en el caso hipot´etico de que H0 no sea rechazada.
c) [8 puntos] Al cabo de un mes, el promedio de ventas para la muestra de vendedores seleccionada fue
de 15.122 autom´oviles por vendedor, con una varianza muestral de 16. Con base en los resultados
del estudio, estime mediante un intervalo al 95 % de confianza el valor medio del volumen de
ventas mensual bajo el nuevo plan de bonificaciones. Haga expl´ıcitos los supuestos en los que basa
su desarrollo.
d) [6 puntos] Dad


---
Exercise- what sample size should I use to get the highest power - minimize type 2 error




Example: compare means in the two samples

Larger price in one group than another. So hypothesis is $\mu_c>\mu_d$  $\mu_c-\mu_d>0$

Let's calculate it in our sample
Go through that test together. Do we reject it? Do we accept it?

Suppose we might think they come from different distributions

-What are hypothesis

-What are test statistics 

-How we calculate the standard error (importance of the independence assumption!)

- case 1: 
  test for large population with unknown variance
  -discuss all possible cases (tails)
  -show graph

- case 2: 
  test for normal populations with known variance

- case 3: 
  test for small sample normal, with uknown variance
  - the difference doesn't really have t distribution..., but it can be approximated
  - calculation of the degrees of freedom (see slides 21)
  
  
  
---
### What if we know that variances are the same? -OPTIONAL?
- we can design a slightly more efficient test - pooled test
- more efficient because we don't have to estimate two variances
which is specifically called the pooled two-sample t-statistic.
• It has an exact t-distribution with m + n − 2 degrees of
freedom when the two populations are normal.
• It is approximately t(m+n−2) for non-normal population w/ equal
SDs as long as the sample size m, n is not too small.
• The degrees of freedom, m + n − 2 is greater than the df of
two-sample t -statistic when σ1 , σ2 (both software formula or
the simple formula)

generally safer to use method with unpooled 
---

### Comparing variances in two samples

**Example Problem 1:** is price dispersion the same in clean or dirty apartments?

**Example Problem 2:** is variance of of prices of two stocks the same?

We need a test for variances in two different samples.


---
### Hypothesis testing with variances

- Suppose you have an iid sample $X_1,X_2,...X_n$ and iid sample $Y_1,Y_2,...Y_m$, both from .blue[normal distributions]
- Denote $s^2_1$ and $s^2_2$ their sample variances and $\sigma^2_1$ and $\sigma^2_2$ their population variances

**Test Statistic** and its distribution under the null is:

Null hypothesis: $\small H_A: \sigma_1 = \sigma_2$
$$F_{test}=\frac{s^2_1}{s^2_2} \sim F_{n-1,m-1}$$


**Rejection region**
1. $\small H_A: \sigma_1 \neq\sigma_2$
  - Reject null if  $\small F_{test}<F_{\frac{\alpha}{2},n-1,m-1}$ or $\small F_{test}>F_{1-\frac{\alpha}{2},n-1,m-1}$

--
2. $\small H_A: \sigma_1 \leq \sigma_2$
  - Reject null if $\small F_{test}<F_{\alpha,n-1,m-1}$

--
3. $\small H_A: \sigma_1 \geq \sigma_2$
  - Reject null if $\small F_{test}>F_{1-\alpha,n-1,m-1}$

--
Where:
- $\small F_{\alpha,n-1,m-1}$ is $\small \alpha$ quantile of $F_{n-1,m-1}$
- So $\small P(F<F_{\alpha,n-1,m-1})=\alpha$
- $\small F_{n-1,m-1}$ distribution is not symmetric around 0!
- So the notation is slightly different!!!!!


---
### F-distribution

If $X_1$ and $X_2$ are chi-square distributed and independent, then the ratio of $X_1$ by its degrees of freedom to $X_2$ by its degrees of freedom is distributed as F. 

$$F=\frac{X1/v_1}{X2/v_2} \sim F_{v1,v2}$$
```{r, warning=FALSE, fig.height=3.5, out.width='100%'}
df_values <- c(3, 10, 100)  # Different degrees of freedom
x_vals <- seq(0, 5, length.out = 400)

# Create a data frame for ggplot
library(ggplot2)

df1 <- data.frame(x = rep(x_vals, length(df_values)),
                 y = c(sapply(df_values, function(df) df(x_vals, df1 = df, df2 = 10))),  # Fix df2 at 10
                 distribution = rep(paste("F-distribution (df1 =", df_values, ", df2 = 10)"), each = length(x_vals)))

df2 <- data.frame(x = rep(x_vals, length(df_values)),
                 y = c(sapply(df_values, function(df) df(x_vals, df1 = df, df2 = 3))),  # Fix df2 at 10
                 distribution = rep(paste("F-distribution (df1 =", df_values, ", df2 = 3)"), each = length(x_vals)))

df=rbind(df1,df2)

# Create ggplot object
gg_plot <- ggplot(df, aes(x = x, y = y, color = distribution)) +
  geom_line() +
  labs(x = "x",
       y = "Density") +
  theme_minimal()

# Convert to plotly object for interactivity
library(plotly)

plotly_object <- ggplotly(gg_plot,
                          width = 800,   # Adjust the width according to your preference
                          height = 400)  # Adjust the height according to your preference

plotly_object

```

---
### Variance test and F-distribution

How come under the null we get F statistics? $s^2$ is not chi square.... but!

$$\frac{(n-1)s^2_1}{\sigma^2_1} \sim \chi_{n-1}$$

$$F_{test}=\frac{\frac{(n-1)s^2_1}{\sigma^2_1(n-1)}}{\frac{(m-1)s^2_2}{\sigma^2_2(m-1)}} \sim F_{n-1,m-1}$$


--

Under null we have that $\sigma_1=\sigma_2$, so


$$F_{test}=\frac{\frac{(n-1)s^2_1}{\sigma^2_1(n-1)}}{\frac{(m-1)s^2_2}{\sigma^2_2(m-1)}}=\frac{s^2_1}{s^2_2}$$

---
**Example:**

Let's test the equality of variances of clean and dirty apartments. 
- $H_0: \sigma^2_c=\sigma^2_d$
- $H_A: \sigma^2_c \neq \sigma^2_d$

--

- Variances in each samples are: $s^2_c=925255.5$ and $s^2_d=480681.9$

- We have 100 observations in each sample, so 99 degrees of freedom on each side

--

$F_test=\frac{s^2_c}{s^2_d}=\frac{925255.5}{480681.9}=1.924$

--

Critical regioins at 5% are:
- $\small F_{\frac{\alpha}{2},n-1,m-1}=F_{0.025,99,99}=0.67284$
- $\small F_{1-\frac{\alpha}{2},n-1,m-1}=F_{0.975,99,99}=1.48623$

--
- We reject the equality at 5%


---
layout: false
class: inverse, middle

# Hypothesis Testing: paired data

---

Paired sample

- one set of individuals, two observations each. 
- Example: They sometimes work at home, sometimes work at the office. You measure their productivity by looking at the hours necessary to complete a project. You want to test which one 

What's the null
What's the alternative


---
Each data consist of indepently chosen pairs, but within pairs y and x can be correlated (which matters for variance and that's why we can't use unpaired test)
test statistic is differences within pairs (standarized)
standard error is  whatever.

So general form is:

For each hypothesis

---

how to estimate variance and choose critical levels?, number of degrees of freedom?

usual cases


In each of the following scenarios, determine if the data are paired?
1. We would like to know if Intel’s stock and Southwest Airlines’
stock have similar rates of return. To find out, we take a
random sample of 50 days, and record Intel’s and Southwest’s
stock on those same days. ⇒ paired
2. We randomly sample 50 items from Target stores and note
the price for each. Then we visit Walmart and collect the price
for each of those same 50 items. ⇒ paired
3. A school board would like to determine whether there is a
difference in average SAT scores for students at one high
school versus another high school in the district. To check,
they take a simple random sample of 100 students from each
high school. ⇒ not paired
---
exercise: Suponga que un psic´ologo piensa que la edad influye en el coeficiente intelectual. Se toma una muestra
aleatoria de 100 personas de mediana edad, de quienes se desconoce su C.I. a la edad de 16 a˜nos y
actualmente. Si al restar los coeficientes de su juventud con los actuales, se obtuvo una diferencia
promedio de 6 puntos, con una desviaci´on est´andar muestral de 7 puntos.
Utilice α = 0.01 para probar la hip´otesis de que el C.I. aumenta con la edad.

---
#### Hypothesis Testing for Correlation

.blue[Example]: are cleanliness and score (C) and price (P) correlated in the population?

Assume you have **normally distributed variables** in **independent pairs**. \{(C_1,P_1), (C_2, P_2),...\} 

.blue[Null hypothesis]: $H_0: \rho(C,P)=0$

**Test statistic** and its distribution under the null:

$$\small T_{test}=\frac{\hat{\rho}(C,P)\sqrt {(n-2)}}{\sqrt{1-\hat{\rho}(C,P)^2}} \sim t_{n-2}$$

where $\hat{\rho}(C,P)$ is sample correlation coefficient


.blue[Alternative hypothesis] and their **rejection regions**:
- $\small H_A: \rho(C,P) \neq 0$ - reject $\small H_0$ if $\small t_{test}>t_{\frac{\alpha}{2},n-2}$ or $\small t_{test}<-t_{\frac{\alpha}{2},n-2}$
- $\small H_A: \rho(C,P)>0$  - reject $\small H_0$ if $\small t_{test}>t_{\alpha,n-2}$
- $\small H_A: \rho(C,P)<0$  - reject $\small H_0$ if $\small t_{test}<t_{\alpha,n-2}$

---
### Correlation test

- Suppose we test $H_0: \rho(C,P)=0$ vs $\rho(C,P)>0$

--

- We assume that:
  - The variables are normally distributed
  - Pairs are independent
  - Relationship would be linear

--

- Sample correlation is: 0.159, we have $n=200$

--

- So $t_{test}=\frac{\hat{\rho}(C,P)\sqrt {(n-2)}}{\sqrt{1-\hat{\rho}(C,P)^2}}=\frac{0.159\sqrt {98}}{\sqrt{1-0.025}}=2.25$

--

- Since $n=200$, t-student is identical to standard normal. We can either
  - Compare calculated statistic to critical regions at various significance levels
  - Compute its p-value
  
- $p-value=P(Z>t_{test})=P(Z>2.25)=0.0126$

--
  - Test at $\alpha=0.05$ would reject $H_0$

---
### Correlation Test 

```{r, warning=FALSE, fig.height=5, out.width='100%'}
ggplot(data=Sample_list, aes(review_scores_cleanliness))+
  geom_histogram()+
  theme_xaringan()

ggplot(data=Sample_list, aes(price))+
  geom_histogram()+
  theme_xaringan()

```



---
### Correlation Test 

```{r, warning=FALSE, fig.height=5, out.width='100%'}

ggplot(data=Sample_list, aes(review_scores_cleanliness, price))+
  geom_point() +
  geom_smooth(method = "loess", fill=NA, level=0.90)+
  theme_xaringan()

```


---
### Exercise

Suppose you have a random sample of 27 units (from a bivariate normal). X measures client's age and Y measures their spending.  You calculated the correlation coefficient of $\hat{\rho}=-0.45$. Can you reject null of $\rho=0$ in favor of alternative $\rho \neq 0$ at 5% significance level?



