[["multiple-linear-regression-model-evaluation-and-hypothesis-testing.html", "2 Multiple Linear Regression: Model Evaluation and Hypothesis Testing 2.1 Introduction 2.2 ANOVA and F-Test for Overall Significance 2.3 Alternative Perspective: Extra Sum of Squares Approach 2.4 Example with Marketing Data 2.5 Testing the sum of the coefficients: 2.6 Application Example: Testing Equality of Facebook and Newspaper Effects", " 2 Multiple Linear Regression: Model Evaluation and Hypothesis Testing 2.1 Introduction In multiple linear regression, we model the relationship between a dependent variable \\(y\\) and multiple independent variables \\(x_1, x_2, \\dots, x_k\\). Our goal is not only to predict \\(y\\) but also to understand how each predictor influences it. In this chapter, we will carefully explore how to evaluate the significance of the overall model and how to test specific hypotheses about individual or groups of coefficients. We’ll use the marketing dataset from the datarium package as an illustrative example. This dataset includes information on the budget allocated to advertising through YouTube, Facebook, and newspaper platforms, along with the corresponding sales generated. library(datarium) ## Warning: package &#39;datarium&#39; was built under R version 4.4.3 data(marketing) head(marketing) ## youtube facebook newspaper sales ## 1 276.12 45.36 83.04 26.52 ## 2 53.40 47.16 54.12 12.48 ## 3 20.64 55.08 83.16 11.16 ## 4 181.80 49.56 70.20 22.20 ## 5 216.96 12.96 70.08 15.48 ## 6 10.44 58.68 90.00 8.64 Businesses frequently invest in advertising across various media channels, and it’s essential to understand which channels effectively drive sales. This understanding helps optimize advertising budgets and maximize return on investment (ROI). 2.2 ANOVA and F-Test for Overall Significance 2.2.1 Why Do We Need the F-Test? The F-test in multiple regression helps us determine whether our model provides a significantly better fit than a model with no predictors. Formally, the null hypothesis is: \\[ H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_k = 0 \\] In words, none of the independent variables have an effect on \\(y\\). Under \\(H_0\\), the model reduces to: \\[ y = \\beta_0 + u \\] which is simply a horizontal line at the mean of \\(y\\). In this case, the regression is useless for explaining \\(y\\). The alternative hypothesis is: \\[ H_A: \\text{At least one } \\beta_j \\neq 0 \\] Rejecting \\(H_0\\) suggests that at least one predictor significantly explains variation in \\(y\\). 2.2.2 Intuition Behind the F-Test If our model is useful, it should explain a large proportion of the total variability in \\(y\\). To evaluate this, we decompose the total variability into: Explained variation (SSR): how much variability the regression explains Unexplained variation (SSE): how much variability remains unexplained (residuals) Thus, we have: \\[ TSS = SSR + SSE \\] where: TSS: Total Sum of Squares (total variation around the mean) SSR: Regression Sum of Squares (variation explained by the model) SSE: Error Sum of Squares (residual variation) 2.2.3 Degrees of Freedom The decomposition and degrees of freedom (df) are summarized in the table: Component | Formula | Interpretation | Degrees of Freedom | |–||–|–| | TSS | \\(\\sum (y_i - \\bar{y})^2\\) | Total variation around the mean | \\(n - 1\\) | | SSR | \\(\\sum (\\hat{y}_i - \\bar{y})^2\\) | Explained variation | \\(k\\) | | SSE | \\(\\sum (y_i - \\hat{y}_i)^2\\) | Unexplained (residual) variation | \\(n - k - 1\\) | Note: - The degrees of freedom add up: \\((k) + (n - k - 1) = (n - 1)\\). - SSR + SSE = TSS. 2.2.4 Understanding the F-Statistic The F-statistic compares the mean explained variation to the mean unexplained variation: \\[ F = \\frac{SSR / k}{SSE / (n - k - 1)} \\] If the model is useless (all \\(\\beta_j = 0\\)), then SSR will be small, and the F-statistic will be close to 1. If the model is useful (some \\(\\beta_j \\neq 0\\)), SSR will be large compared to SSE, and the F-statistic will be significantly greater than 1. 2.2.5 Intuitive Explanation of the F-Test If \\(H_0\\) is true, \\(F\\) should be small. A large F-statistic provides evidence to reject \\(H_0\\). The p-value associated with the F-statistic is: \\(p = P(F_{k, n-k-1} &gt; F_{\\text{observed}})\\) where \\(F_{k, n-k-1}\\) denotes the F-distribution with \\(k\\) and \\(n-k-1\\) degrees of freedom. Important: - This is a one-sided test. - We only reject \\(H_0\\) if the F-statistic is sufficiently large (no need to double the p-value). 2.3 Alternative Perspective: Extra Sum of Squares Approach Another way to understand the F-test is to think in terms of comparing two models: Restricted model (under \\(H_0\\)): only includes the intercept. \\[ H_0: y = \\beta_0 + u \\] Unrestricted model (under \\(H_A\\)): includes all predictors. \\[ H_A: y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + u \\] If \\(H_A\\) is true, the unrestricted model should explain significantly more variation than the restricted model. Thus, the F-statistic can also be written as: \\[ F = \\frac{ \\left( \\text{SSE}_{\\text{restricted}} - \\text{SSE}_{\\text{unrestricted}} \\right) / (k - k_0) }{ \\text{SSE}_{\\text{unrestricted}} / (n - k) } \\] where: \\(\\text{SSE}_{\\text{restricted}}\\): residual sum of squares from the restricted model (intercept only) \\(\\text{SSE}_{\\text{unrestricted}}\\): residual sum of squares from the full model \\(k_0\\): number of predictors in the restricted model (typically 0) Because the restricted model explains nothing (only the mean), \\(SSR_{H_0} = 0\\), and the difference simplifies to \\(SSR\\) of the unrestricted model. Thus, this “extra sum of squares” formulation is mathematically equivalent to the earlier F-formula based on explained and unexplained variation. 2.4 Example with Marketing Data Let’s apply our detailed understanding of multiple regression analysis using the marketing dataset from the datarium package. Specifically, we want to examine how advertising expenditures on YouTube, Facebook, and newspapers influence product sales. We will estimate the following regression equation: \\[ sales_i = \\beta_0 + \\beta_1 \\, youtube_i + \\beta_2 \\, facebook_i + \\beta_3 \\, newspaper_i + u_i \\] Each coefficient \\(\\beta\\) measures the change in sales associated with each additional dollar invested in the respective advertising platform, holding the others constant. 2.4.1 Step-by-Step Regression in R Here’s how we perform this regression analysis using R: # Fit the multiple regression model model &lt;- lm(sales ~ youtube + facebook + newspaper, data = marketing) # Display detailed summary summary(model) ## ## Call: ## lm(formula = sales ~ youtube + facebook + newspaper, data = marketing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.5932 -1.0690 0.2902 1.4272 3.3951 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.526667 0.374290 9.422 &lt;2e-16 *** ## youtube 0.045765 0.001395 32.809 &lt;2e-16 *** ## facebook 0.188530 0.008611 21.893 &lt;2e-16 *** ## newspaper -0.001037 0.005871 -0.177 0.86 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.023 on 196 degrees of freedom ## Multiple R-squared: 0.8972, Adjusted R-squared: 0.8956 ## F-statistic: 570.3 on 3 and 196 DF, p-value: &lt; 2.2e-16 2.4.2 Interpreting the Model Output Let’s carefully interpret the output: *Coefficients:** Intercept: 3.5267 — the expected sales when there is no spending on any platform. YouTube coefficient: 0.0458 — each additional dollar spent on YouTube advertising increases sales by approximately 0.0458 units, holding Facebook and newspaper spending constant. Facebook coefficient: 0.1885 — each additional dollar spent on Facebook advertising increases sales by about 0.1885 units, ceteris paribus. Newspaper coefficient: -0.0010 — spending on newspaper advertising shows a very small, statistically insignificant, negative effect on sales. Statistical significance: YouTube and Facebook coefficients are highly statistically significant (p-values &lt; 0.001), indicating strong evidence that these platforms positively influence sales. Newspaper advertising is not statistically significant (p-value ≈ 0.86), suggesting no effect. 2.4.3 ANOVA Table for Detailed Insights The Analysis of Variance (ANOVA) table summarizes how much of the variability in sales is explained by the model. The anova() function in R provides a decomposition of sum of squares for each predictor separately. To get the total explained variation (SSR), we add the sum of squares for YouTube, Facebook, and Newspaper. anova(model) ## Analysis of Variance Table ## ## Response: sales ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## youtube 1 4773.1 4773.1 1166.7308 &lt;2e-16 *** ## facebook 1 2225.7 2225.7 544.0501 &lt;2e-16 *** ## newspaper 1 0.1 0.1 0.0312 0.8599 ## Residuals 196 801.8 4.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note: - R reports the Sum of Squares for each predictor separately (e.g., YouTube, Facebook, Newspaper). - Adding them together gives the total explained sum of squares (SSR). To obtain a simplified classic ANOVA table (model vs residuals), we can use a custom function like simpleAnova(model) (assuming it’s pre-written). simpleAnova(model) ## Analysis of Variance Table ## ## Response: sales ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Predictors 3 6998.9 2332.96 570.27 &lt; 2.2e-16 *** ## Residuals 196 801.8 4.09 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.4.4 Understanding the ANOVA Output From the ANOVA table: Sum of Squares Regression (SSR): Add contributions from YouTube, Facebook, and Newspaper. SSR ≈ 6,998.9 Sum of Squares Error (SSE): Residuals ≈ 801.8 Degrees of Freedom: Regression df = 3 (number of predictors) Residual df = 196 (total observations - predictors - 1) Mean Squares: MSR = SSR / df_regression MSE = SSE / df_residuals F-statistic: Reported as 570.3 P-value: Less than 2.2e-16 (very strong evidence against \\(H_0\\)). 2.4.5 Clearly Formulated Hypotheses Let’s explicitly write the ANOVA F-test hypotheses: \\[ H_0: \\beta_{\\text{youtube}} = \\beta_{\\text{facebook}} = \\beta_{\\text{newspaper}} = 0 \\] \\[ H_A: \\text{At least one } \\beta_j \\neq 0 \\] Decision rule: - Reject \\(H_0\\) if the F-statistic is larger than the critical F-value (or if the p-value is smaller than 0.05). - Rejecting \\(H_0\\) confirms that advertising expenditures (at least on one platform) significantly affect sales. 2.4.6 Distribution of the F-Statistic Under the Null Given that we have 200 observations, under \\(H_0\\) (no advertising effect): Numerator degrees of freedom = 3 (predictors) Denominator degrees of freedom = \\(n - k - 1 = 200 - 3 - 1 = 196\\) At a 5% significance level, the critical value of the F-distribution \\(F(3, 196)\\) is approximately 2.65. 2.4.7 Calculating the F-Statistic Manually Recall: \\[ F = \\frac{SSR/k}{SSE/(n-k-1)} \\] Substituting the numbers: \\[ F = \\frac{6998.9/3}{801.8/196} = 570.3 \\] However, because in R’s calculation each contribution to SSR is added separately, and model fitting automatically adjusts for estimation, the reported F-statistic (570.3) corresponds correctly to the model summary and internal variance estimates. (You can double-check by looking at Mean Squares: MSR / MSE.) 2.4.8 Conclusion The calculated F-statistic (570.3) is far larger than the critical value (2.65). The p-value is extremely small (0). Thus, we strongly reject the null hypothesis. Conclusion: Advertising spending on at least one platform significantly influences product sales. 2.4.9 Relationships between coefficients Sometimes we are interested in testing the relationships between coefficients in the model. That allows us to compare effects of different variables or look at their sums. Consider a model: \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + u \\] 2.4.10 Difference between coefficients We are interested in testing whether the difference between the impact of \\(x_1\\) and \\(x_2\\) is equal to a constant \\(c\\). Hypotheses: - \\(H_0: \\beta_1 - \\beta_2 = c\\) - \\(H_A: \\beta_1 - \\beta_2 \\neq c\\) A special case is when \\(c = 0\\), meaning we are testing whether the coefficients are equal: \\[ H_0: \\beta_1 - \\beta=0 \\] Which is the same as: \\[ H_0: \\beta_1 = \\beta_2 \\] 2.4.10.1 Test Statistic and Its Distribution Under the Null The test statistic is: \\[ T_{\\text{test}} = \\frac{ \\hat{\\beta}_1 - \\hat{\\beta}_2 - c }{ SE(\\hat{\\beta}_1 - \\hat{\\beta}_2) } \\] where \\[ SE(\\hat{\\beta}_1 - \\hat{\\beta}_2) = \\sqrt{ \\text{Var}(\\hat{\\beta}_1) + \\text{Var}(\\hat{\\beta}_2) - 2\\text{Cov}(\\hat{\\beta}_1, \\hat{\\beta}_2) } \\] Under \\(H_0\\), the test statistic follows a t-distribution: \\[ T_{\\text{test}} \\sim t_{n-k-1} \\] where: - \\(n\\) is the number of observations - \\(k\\) is the number of predictors (excluding intercept) We calculate the p-value as: \\[ \\text{p-value} = 2P(t_{n-k-1} &gt; |T_{\\text{test}}|) \\] 2.4.11 Testing Inequalities Between Coefficients Similarly, we can test whether one coefficient is greater than another by a given amount. Hypotheses: - \\(H_0: \\beta_1 - \\beta_2 = c\\) - \\(H_A: \\beta_1 - \\beta_2 &gt; c\\) Special case when \\(c = 0\\): testing whether \\(\\beta_1 &gt; \\beta_2\\). 2.4.11.1 Test Statistic and p-Value The test statistic remains: \\[ T_{\\text{test}} = \\frac{ \\hat{\\beta}_1 - \\hat{\\beta}_2 - c }{ SE(\\hat{\\beta}_1 - \\hat{\\beta}_2) } \\] and under \\(H_0\\): \\[ T_{\\text{test}} \\sim t_{n-k-1} \\] The p-value depends on the direction of the alternative hypothesis: For \\(H_A: \\beta_1 - \\beta_2 &gt; c\\): \\[ \\text{p-value} = P(t_{n-k-1} &gt; T_{\\text{test}}) \\] For \\(H_A: \\beta_1 - \\beta_2 &lt; c\\): \\[ \\text{p-value} = P(t_{n-k-1} &lt; T_{\\text{test}}) \\] 2.5 Testing the sum of the coefficients: We are interested in testing whether the sum of the impacts of \\(x_1\\) and \\(x_2\\) is equal to a constant \\(c\\). Hypotheses: - \\(H_0: \\beta_1 + \\beta_2 = c\\) - \\(H_A: \\beta_1 + \\beta_2 \\neq c\\) Helps to answer Is combined effect bigger than some number? Does increasing both at the same time has positive or negative effect (if coefficients have opposite signs)? \\[ H_0: \\beta_1 + \\beta=0 \\] vs \\[ H_A: \\beta_1 + \\beta \\neq 0 \\] or for one sided tests: \\[ H_A: \\beta_1 + \\beta &gt;0 \\] or \\[ H_A: \\beta_1 + \\beta &lt;0 \\] 2.5.0.1 Test Statistic and Its Distribution Under the Null The test statistic is: \\[ T_{\\text{test}} = \\frac{ \\hat{\\beta}_1 + \\hat{\\beta}_2 - c }{ SE(\\hat{\\beta}_1 + \\hat{\\beta}_2) } \\] where (sings on covariance changes): \\[ SE(\\hat{\\beta}_1 + \\hat{\\beta}_2) = \\sqrt{ \\text{Var}(\\hat{\\beta}_1) + \\text{Var}(\\hat{\\beta}_2) + 2\\text{Cov}(\\hat{\\beta}_1, \\hat{\\beta}_2) } \\] Under \\(H_0\\), the test statistic follows a t-distribution: \\[ T_{\\text{test}} \\sim t_{n-k-1} \\] where: - \\(n\\) is the number of observations - \\(k\\) is the number of predictors (excluding intercept) We calculate the p-value for two sided test as: \\[ \\text{p-value} = 2P(t_{n-k-1} &gt; |T_{\\text{test}}|) \\] And for one sided test we don’t mulitply by 2. 2.6 Application Example: Testing Equality of Facebook and Newspaper Effects Suppose we want to test whether the effect of Facebook advertising is equal to the effect of Newspaper advertising. As a reminder: Coefficient | Estimate | Std. Error | |–|-|| | (Intercept) | 3.526667 | 0.374290 | | YouTube | 0.045765 | 0.001395 | | Facebook | 0.188530 | 0.008611 | | Newspaper | -0.001037 | 0.005871 | Formally: \\[ H_0: \\beta_{\\text{facebook}} = \\beta_{\\text{newspaper}} \\] This is equivalent to testing: \\[ H_0: \\beta_{\\text{facebook}} - \\beta_{\\text{newspaper}} = 0 \\] against the alternative: \\[ H_A: \\beta_{\\text{facebook}} - \\beta_{\\text{newspaper}} \\neq 0 \\] 2.6.1 Manual Calculation: Step-by-Step We can calculate the test manually. Extract the variance-covariance matrix of estimated coefficients: vcov(model) ## (Intercept) youtube facebook newspaper ## (Intercept) 0.1400929170 -3.188728e-04 -1.338587e-03 -7.092255e-04 ## youtube -0.0003188728 1.945737e-06 -4.470395e-07 -3.265950e-07 ## facebook -0.0013385874 -4.470395e-07 7.415335e-05 -1.780062e-05 ## newspaper -0.0007092255 -3.265950e-07 -1.780062e-05 3.446875e-05 \\(\\text{Var}(\\hat{\\beta}_{\\text{facebook}}) = 7.415335 \\times 10^{-5}\\) \\(\\text{Var}(\\hat{\\beta}_{\\text{newspaper}}) = 3.446875 \\times 10^{-5}\\) \\(\\text{Cov}(\\hat{\\beta}_{\\text{facebook}}, \\hat{\\beta}_{\\text{newspaper}}) = -1.780062 \\times 10^{-5}\\) Remember: In the variance-covariance matrix, the first row/column corresponds to the intercept! 2.6.1.1 Step 1: Calculate the Standard Error We use the formula: \\[ SE(\\hat{\\beta}_{\\text{facebook}} - \\hat{\\beta}_{\\text{newspaper}}) = \\sqrt{ \\text{Var}(\\hat{\\beta}_{\\text{facebook}}) + \\text{Var}(\\hat{\\beta}_{\\text{newspaper}}) - 2\\text{Cov}(\\hat{\\beta}_{\\text{facebook}}, \\hat{\\beta}_{\\text{newspaper}}) } \\] Substituting values: \\[ SE = \\sqrt{ (7.415335 \\times 10^{-5}) + (3.446875 \\times 10^{-5}) - 2(-1.780062 \\times 10^{-5}) } \\] \\[ SE = \\sqrt{ (7.415335 + 3.446875 - 2 \\times -1.780062) \\times 10^{-5} } \\] \\[ SE = \\sqrt{ 0.0001442233 } \\] \\[ SE \\approx 0.0120093 \\] Thus: \\[ SE(\\hat{\\beta}_{\\text{facebook}} - \\hat{\\beta}_{\\text{newspaper}}) \\approx 0.0120093 \\] 2.6.1.2 Step 2: Calculate the Test Statistic The formula for the test statistic is: \\[ T_{\\text{test}} = \\frac{ \\hat{\\beta}_{\\text{facebook}} - \\hat{\\beta}_{\\text{newspaper}} - 0 }{ SE(\\hat{\\beta}_{\\text{facebook}} - \\hat{\\beta}_{\\text{newspaper}}) } \\] Substituting in the estimates: \\[ T_{\\text{test}} = \\frac{ 0.188530 - (-0.001037) }{ 0.0120093 } \\] \\[ T_{\\text{test}} = \\frac{ 0.189567 }{ 0.0120093 } \\] \\[ T_{\\text{test}} \\approx 15.78 \\] 2.6.1.3 Step 3: Find the p-value Under \\(H_0\\), the test statistic follows a \\(t\\)-distribution with \\(n - k - 1\\) degrees of freedom. In this case: - \\(n = 200\\) observations - \\(k = 3\\) predictors (YouTube, Facebook, Newspaper) Thus: \\[ \\text{Degrees of Freedom} = 200 - 3 - 1 = 196 \\] The p-value for a two-sided test is: \\[ \\text{p-value} = 2P(t_{196} &gt; 15.78) \\] Given that 15.78 is extremely large, the p-value is effectively close to zero (\\(&lt; 0.0001\\)). Thus, we strongly reject the null hypothesis that the effect of Facebook equals the effect of Newspaper. 2.6.2 Additional example: Comparing 4 Dollars in YouTube to 1 Dollar in Facebook We want to test whether spending 4 dollars on YouTube has the same effect as spending 1 dollar on Facebook. In other words, we want to test: \\[ H_0: 4\\beta_{\\text{youtube}} = \\beta_{\\text{facebook}} \\] This can be rearranged into the standard form: \\[ H_0: 4\\beta_{\\text{youtube}} - \\beta_{\\text{facebook}} = 0 \\] Alternative Hypothesis: \\[ H_A: 4\\beta_{\\text{youtube}} - \\beta_{\\text{facebook}} \\neq 0 \\] 2.6.2.1 Step 1: Calculate the Estimate of the Linear Combination From the regression output: \\(\\hat{\\beta}_{\\text{youtube}} = 0.045765\\) \\(\\hat{\\beta}_{\\text{facebook}} = 0.188530\\) Thus: \\[ 4\\hat{\\beta}_{\\text{youtube}} - \\hat{\\beta}_{\\text{facebook}} = 4(0.045765) - 0.188530 \\] \\[ = 0.18306 - 0.188530 = -0.00547 \\] 2.6.2.2 Step 2: Calculate the Standard Error The variance of the linear combination \\(4\\hat{\\beta}_{\\text{youtube}} - \\hat{\\beta}_{\\text{facebook}}\\) is: \\[ \\text{Var}(4\\hat{\\beta}_{\\text{youtube}} - \\hat{\\beta}_{\\text{facebook}}) = 4^2 \\times \\text{Var}(\\hat{\\beta}_{\\text{youtube}}) + (-1)^2 \\times \\text{Var}(\\hat{\\beta}_{\\text{facebook}}) + 2 \\times 4 \\times (-1) \\times \\text{Cov}(\\hat{\\beta}_{\\text{youtube}}, \\hat{\\beta}_{\\text{facebook}}) \\] Substituting the given values from above variance covariance matrix: \\[ = 16 \\times 1.945737\\times 10^{-6} + 1 \\times 7.415335\\times 10^{-5} + 2 \\times 4 \\times (-1) \\times (-4.470395\\times 10^{-7}) \\] \\[ = 16 \\times 0.000001945737 + 0.00007415335 + 2 \\times 4 \\times 0.0000004470395 \\] \\[ = 0.000031131792 + 0.00007415335 + 0.000003576316 \\] \\[ = 0.00010886146 \\] Thus: \\[ SE = \\sqrt{0.00010886146} \\approx 0.01043 \\] 2.6.2.3 Step 3: Calculate the Test Statistic \\[ T_{\\text{test}} = \\frac{-0.00547 - 0}{0.01043} \\approx -0.524 \\] 2.6.2.4 Step 4: Find the p-value Degrees of freedom = \\(200 - 3 - 1 = 196\\) The p-value for a two-sided test is: \\[ \\text{p-value} = 2P(t_{196} &gt; 0.524)=0.7 \\] Since \\(T_{\\text{test}}\\) is small in absolute value, the p-value will be large (&gt;0.5). Conclusion: We fail to reject \\(H_0\\). There is no evidence that the effect of 4 dollars in YouTube is different from 1 dollar in Facebook. 2.6.3 Example 2: Testing if the Combined Effect of YouTube and Facebook Exceeds 0.22 Now, we want to test whether the sum of the effects of 1 dollar in YouTube and 1 dollar in Facebook is greater than 0.22. Formally: \\[ H_0: \\beta_{\\text{youtube}} + \\beta_{\\text{facebook}} = 0.22 \\] \\[ H_A: \\beta_{\\text{youtube}} + \\beta_{\\text{facebook}} &gt; 0.22 \\] 2.6.3.1 Step 1: Calculate the Estimate of the Linear Combination \\[ \\hat{\\beta}_{\\text{youtube}} + \\hat{\\beta}_{\\text{facebook}} = 0.045765 + 0.188530 = 0.234295 \\] 2.6.3.2 Step 2: Calculate the Standard Error The variance of the linear combination \\(\\hat{\\beta}_{\\text{youtube}} + \\hat{\\beta}_{\\text{facebook}}\\) is: \\[ \\text{Var}(\\hat{\\beta}_{\\text{youtube}} + \\hat{\\beta}_{\\text{facebook}}) = \\text{Var}(\\hat{\\beta}_{\\text{youtube}}) + \\text{Var}(\\hat{\\beta}_{\\text{facebook}}) + 2\\text{Cov}(\\hat{\\beta}_{\\text{youtube}}, \\hat{\\beta}_{\\text{facebook}}) \\] Substituting values: \\[ = 1.945737\\times 10^{-6} + 7.415335\\times 10^{-5} + 2 \\times (-4.470395\\times 10^{-7}) \\] \\[ = 0.000001945737 + 0.00007415335 - 0.000000894079 \\] \\[ = 0.000075205008 \\] Thus: \\[ SE = \\sqrt{0.000075205008} \\approx 0.008673 \\] 2.6.3.3 Step 3: Calculate the Test Statistic \\[ T_{\\text{test}} = \\frac{0.234295 - 0.22}{0.008673} = \\frac{0.014295}{0.008673} \\approx 1.648 \\] 2.6.3.4 Step 4: Find the p-value Since this is a one-sided test (greater than), we compute: \\[ \\text{p-value} = P(t_{196} &gt; 1.648)=0.0504 \\] Consulting the t-distribution, the p-value is approximately 0.05. Conclusion: - The p-value is slightly above 0.05. - Thus, there is no evidence to reject \\(H_0\\) at the 5% level, but you could reject at 10% - Interpretation: There is weak evidence that the combined effect exceeds 0.22. Or you could say there is no strong evidence to reject. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
